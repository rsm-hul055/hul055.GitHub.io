---
title: "HW4 Part 1: Unsupervised Learning for Customer Segmentation"
subtitle: "Clustering with Bill and Flipper Length to Segment Penguins"
author: "Hui Liu"
date: today
format: html
jupyter: python3
callout-appearance: minimal
---

## 1a. K-Means Clustering on Palmer Penguins

This analysis demonstrates how unsupervised learning can be used to segment customers (or in this case, penguins) based on physical characteristics, which is analogous to segmenting markets in marketing analytics.

### Data Preparation

We used the Palmer Penguins dataset and selected two numerical features:
- `bill_length_mm`
- `flipper_length_mm`

After removing missing values, we applied our custom implementation of the K-Means clustering algorithm.

::: {.callout-note collapse=true title="Read Data"}
```{python echo=False}
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("./palmer_penguins.csv")


df_clean = df.dropna(subset=["bill_length_mm", "flipper_length_mm"])


X = df_clean[["bill_length_mm", "flipper_length_mm"]].values

df_clean.head()
```
:::


### Custom K-Means Implementation

We implemented the K-Means algorithm from scratch, using Euclidean distance for assignment and mean updates for centroid repositioning. The algorithm converged after 9 iterations.

Below is a **GIF animation** of the clustering process:

![Evaluation Metrics](evaluation_plot.png)

::: {.callout-note collapse=true title="Custom K-Means Algorithm Implementation"}
```{python echo=False}
import numpy as np
import os

# Set random seed
np.random.seed(42)

# Create a folder to save the step images
frame_dir = "./kmeans_frames"
os.makedirs(frame_dir, exist_ok=True)

# Helper: compute Euclidean distance
def euclidean_distance(a, b):
    return np.linalg.norm(a - b, axis=1)

# Save each step's plot
def save_kmeans_step_plot(X, labels, centroids, step, frame_dir):
    plt.figure(figsize=(6, 5))
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set2', s=40, edgecolor='k')
    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=120, marker='x', linewidths=2)
    plt.title(f"K-Means Iteration {step}")
    plt.xlabel("Bill Length (mm)")
    plt.ylabel("Flipper Length (mm)")
    plt.tight_layout()
    plt.savefig(f"{frame_dir}/frame_{step:02d}.png")
    plt.close()

# Custom K-Means implementation with visualization saving
def custom_kmeans(X, k=3, max_iter=10, save_dir=None):
    n_samples, n_features = X.shape
    initial_indices = np.random.choice(n_samples, k, replace=False)
    centroids = X[initial_indices]
    labels = np.zeros(n_samples)
    
    for step in range(max_iter):
        # Assign clusters
        for i in range(n_samples):
            distances = euclidean_distance(X[i].reshape(1, -1), centroids)
            labels[i] = np.argmin(distances)
        
        # Save current step
        if save_dir:
            save_kmeans_step_plot(X, labels, centroids, step, save_dir)
        
        # Compute new centroids
        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])
        
        # Check for convergence
        if np.allclose(centroids, new_centroids):
            print(f"Converged at step {step}")
            break
        
        centroids = new_centroids
    
    # Final plot
    if save_dir:
        save_kmeans_step_plot(X, labels, centroids, step + 1, save_dir)

    return labels, centroids

# Run custom K-Means and save frames
labels, centroids = custom_kmeans(X, k=3, max_iter=10, save_dir=frame_dir)



```
:::


::: {.callout-note collapse=true title="Create GIF Animation with imageio"}
```{python echo=False}
import imageio

# Generate GIF from saved frames
frame_files = sorted([os.path.join(frame_dir, f) for f in os.listdir(frame_dir) if f.endswith(".png")])
gif_path = "./kmeans_animation.gif"

# Create gif
with imageio.get_writer(gif_path, mode='I', duration=0.6) as writer:
    for filename in frame_files:
        image = imageio.imread(filename)
        writer.append_data(image)

gif_path




```
:::


::: {.callout-note collapse=true title="Evaluate Cluster Quality (WCSS & Silhouette)"}
```{python echo=False}

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

wcss = []
silhouette_scores = []
K_range = range(2, 8)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    wcss.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, labels))

# Plot WCSS and Silhouette Scores
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

ax[0].plot(K_range, wcss, marker='o')
ax[0].set_title('Within-Cluster Sum of Squares (WCSS)')
ax[0].set_xlabel('Number of Clusters (K)')
ax[0].set_ylabel('WCSS')

ax[1].plot(K_range, silhouette_scores, marker='o', color='green')
ax[1].set_title('Silhouette Score')
ax[1].set_xlabel('Number of Clusters (K)')
ax[1].set_ylabel('Silhouette Score')

plt.tight_layout()
plt.savefig("evaluation_plot.png")
plt.show()



```
:::


### Choosing the Number of Clusters

To determine the optimal number of clusters (K), we computed:

- **Within-Cluster Sum of Squares (WCSS)**
- **Silhouette Score**

Both metrics suggested **K = 3** as the best choice:

![Sklearn Clustering](sklearn_kmeans.png)

::: {.callout-note collapse=true title="Scikit-learn KMeans Comparison"}
```{python echo=False}

# Fit KMeans from scikit-learn for K=3
kmeans_sklearn = KMeans(n_clusters=3, random_state=42, n_init=10)
labels_sklearn = kmeans_sklearn.fit_predict(X)
centroids_sklearn = kmeans_sklearn.cluster_centers_

# Plot sklearn KMeans result
plt.figure(figsize=(6, 5))
plt.scatter(X[:, 0], X[:, 1], c=labels_sklearn, cmap='Set2', s=40, edgecolor='k')
plt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], c='black', s=120, marker='x', linewidths=2)
plt.title("K-Means Clustering (scikit-learn)")
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.tight_layout()
plt.savefig("sklearn_kmeans.png")
plt.show()


```
:::

### Comparison with Scikit-learn's KMeans

To validate our custom implementation, we compared the clustering result with `scikit-learn`'s built-in `KMeans` function:

![Sklearn Clustering](sklearn_kmeans.png)

The cluster assignments and centroid locations are nearly identical, confirming the correctness of our implementation.