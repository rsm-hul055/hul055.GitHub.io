---
title: "HW4 Part 2 (2a): Supervised Learning for Predictive Modeling"
subtitle: "Using K-Nearest Neighbors to Predict Customer Satisfaction"
author: "Hui Liu"
date: today
format: html
jupyter: python3
callout-appearance: minimal
---
## 2a. K Nearest Neighbors

This exercise implements the K-Nearest Neighbors algorithm from scratch and evaluates its performance on a synthetic classification task with a nonlinear boundary.


::: {.callout-note collapse=true title="Generate Training Data and Test Data"}
```{python echo=False}

# Re-execute after kernel reset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate training data
np.random.seed(42)
n_train = 100
x1_train = np.random.uniform(-3, 3, n_train)
x2_train = np.random.uniform(-3, 3, n_train)
boundary_train = np.sin(4 * x1_train) + x1_train
y_train = (x2_train > boundary_train).astype(int)

train_data = pd.DataFrame({
    "x1": x1_train,
    "x2": x2_train,
    "y": y_train
})

# Generate test data
np.random.seed(99)
n_test = 100
x1_test = np.random.uniform(-3, 3, n_test)
x2_test = np.random.uniform(-3, 3, n_test)
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = (x2_test > boundary_test).astype(int)

test_data = pd.DataFrame({
    "x1": x1_test,
    "x2": x2_test,
    "y": y_test
})
```
:::

### Visualizing the Training Data

We generate a dataset of 100 points where each observation has two features (`x1`, `x2`).  
The class label `y` is determined by whether the point lies above or below the nonlinear decision boundary:

\[
x_2 = \sin(4x_1) + x_1
\]

The plot below illustrates the decision boundary and class distribution:

![Synthetic Training Data](knn_train_plot.png)


::: {.callout-note collapse=true title="Plot training data and wiggly decision boundary"}

```{python echo=False}

plt.figure(figsize=(6, 5))
plt.scatter(x1_train, x2_train, c=y_train, cmap='coolwarm', s=40, edgecolor='k')
x_curve = np.linspace(-3, 3, 300)
y_curve = np.sin(4 * x_curve) + x_curve
plt.plot(x_curve, y_curve, color='black', linestyle='--', label='Decision Boundary')
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Synthetic Training Data with Wiggly Boundary")
plt.legend()
plt.tight_layout()
train_plot_path = "./knn_train_plot.png"
plt.savefig(train_plot_path)
plt.close()

train_plot_path

```
:::


### Implementing and Evaluating KNN

We implemented KNN from scratch. For each point in the test set (also of size 100), we computed its K nearest neighbors in the training set and classified it by majority vote.

We evaluated model accuracy for \( k = 1 \) through \( k = 30 \), and the results are summarized below:

![KNN Accuracy Curve](knn_accuracy_curve_labeled.png)

As shown, the accuracy peaks at **K = 1** with an accuracy of **92%**.  
This suggests that in this setting, the 1-nearest neighbor already captures the complex boundary quite well.

::: {.callout-note collapse=true title="Implement custom KNN classifier"}
```{python echo=False}

from scipy.spatial import distance

def knn_predict(X_train, y_train, X_test, k=3):
    y_pred = []
    for test_point in X_test:
        dists = distance.cdist([test_point], X_train)[0]
        nearest_indices = np.argsort(dists)[:k]
        nearest_labels = y_train[nearest_indices]
        majority_vote = np.argmax(np.bincount(nearest_labels))
        y_pred.append(majority_vote)
    return np.array(y_pred)

# Run accuracy for k=1 to 30
X_train = train_data[["x1", "x2"]].values
y_train = train_data["y"].values
X_test = test_data[["x1", "x2"]].values
y_test = test_data["y"].values

accuracies = []
for k in range(1, 31):
    preds = knn_predict(X_train, y_train, X_test, k=k)
    acc = (preds == y_test).mean()
    accuracies.append(acc)

# Plot
optimal_k = np.argmax(accuracies) + 1
optimal_acc = accuracies[optimal_k - 1]

plt.figure(figsize=(8, 5))
plt.plot(range(1, 31), accuracies, marker='o')
plt.axvline(optimal_k, color='red', linestyle='--', label=f'Optimal k = {optimal_k}')
plt.scatter(optimal_k, optimal_acc, color='red')
plt.title("KNN Accuracy on Test Set")
plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("knn_accuracy_curve_labeled.png")
plt.close()

```
:::


### Final Notes

Unlike tree-based models or regression, K-Nearest Neighbors does not produce variable importance metrics.  
Because it makes decisions purely based on distance in feature space, both features (`x1`, `x2`) contribute jointly to the classification task.

If one wants to evaluate feature contributions explicitly, alternative models like random forests, logistic regression, or SHAP-enhanced models may be better suited.

::: {.callout-note collapse=true title="KNN Algorithm Parameters"}
The table below summarizes key parameters used in our custom KNN implementation:

| Parameter              | Value                             |
|------------------------|-----------------------------------|
| Distance Metric        | Euclidean                         |
| Training Set Size      | 100                               |
| Test Set Size          | 100                               |
| Feature Dimensions     | 2                                 |
| K Range Tested         | 1 to 30                           |
| Optimal K              | 1                                 |
| Decision Boundary      | \( x_2 = \sin(4x_1) + x_1 \)      |
| Classification Method  | Majority vote (mode of k nearest) |
:::