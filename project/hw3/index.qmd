---
title: "Multinomial Logit Model"
author: "Hui Liu"
date: today
jupyter: python3
callout-appearance: minimal 
---


This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.

::: {.callout-note collapse=true title="Read Data"}
```{python echo=False}
import pandas as pd

df = pd.read_csv("./conjoint_data.csv")

df.info(), df.head()
```
:::

### Data Summary

| Column   | Description                                                   |
|----------|---------------------------------------------------------------|
| `resp`   | Respondent ID                                                 |
| `task`   | Task number answered by each respondent (3 options per task)  |
| `choice` | Whether the option was chosen (1 = chosen, 0 = not chosen)    |
| `brand`  | Brand (categorical: N, H, P)                                  |
| `ad`     | Whether the product has advertising (Yes/No)                  |
| `price`  | Product price                                                 |

### One-hot Encode Categorical Variables
```{python}
# One-hot encode brand and ad
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)

# Check if each (resp, task) pair has 3 alternatives
group_counts = df_encoded.groupby(['resp', 'task']).size().value_counts()

# Preview reshaped data
df_encoded.head(), group_counts
```

We one-hot encode the categorical variables brand and ad to prepare them for use in the multinomial logit model, which requires numeric input features.
We also confirm that each respondent-task pair includes exactly 3 alternatives, ensuring proper setup for MNL estimation.
### The dataset is now encoded and validated, ready for modeling.

## 4. Estimation via Maximum Likelihood

We estimate the parameters of the multinomial logit (MNL) model using maximum likelihood, via `scipy.optimize.minimize` with the BFGS method.

The log-likelihood function is constructed over respondent-task-choice groups, treating the utility of each product as a linear function of the features:

$$
U_{ij} = X_{ij} \beta
$$

We compute the negative log-likelihood, then solve for parameter estimates by minimizing this objective. We also compute the inverse Hessian to obtain standard errors and 95% confidence intervals.

::: {.callout-note title="Maximum Likelihood Estimation Results" collapse=true}

```{python echo=False}
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from numpy.linalg import inv

# Load and encode data
df = pd.read_csv("./conjoint_data.csv")
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)

# Define X and y
X = df_encoded[["price", "brand_N", "brand_P", "ad_Yes"]].values
y = df_encoded["choice"].values
groups = df_encoded[["resp", "task"]].values

# Ensure 3 alternatives per task
group_sizes = pd.DataFrame(groups, columns=["resp", "task"]).value_counts().values
assert all(group_sizes == 3)

# Reshape
X_grouped = X.reshape((-1, 3, X.shape[1]))
y_grouped = y.reshape((-1, 3))

# Ensure correct types
X_grouped = np.asarray(X_grouped, dtype=np.float64)
y_grouped = np.asarray(y_grouped, dtype=np.float64)

# Define log-likelihood
def neg_log_likelihood(beta):
    beta = np.asarray(beta, dtype=np.float64)
    utilities = np.dot(X_grouped, beta)  # Shape: (N_tasks, 3)
    exp_util = np.exp(utilities)
    probs = exp_util / exp_util.sum(axis=1, keepdims=True)
    chosen_probs = (probs * y_grouped).sum(axis=1)
    log_likelihood = np.log(chosen_probs + 1e-12).sum()
    return -log_likelihood

# Estimate
beta0 = np.zeros(X.shape[1])
result = minimize(neg_log_likelihood, beta0, method="BFGS")
beta_hat = result.x
hessian_inv = result.hess_inv

# Standard errors and confidence intervals
se = np.sqrt(np.diag(hessian_inv))
ci_lower = beta_hat - 1.96 * se
ci_upper = beta_hat + 1.96 * se

# Output results
results_df = pd.DataFrame({
    "Parameter": ["price", "brand_N", "brand_P", "ad_Yes"],
    "Estimate": beta_hat,
    "Std. Error": se,
    "95% CI Lower": ci_lower,
    "95% CI Upper": ci_upper
})
results_df
```
:::


The table below shows the Maximum Likelihood Estimates, standard errors, and 95% confidence intervals for each parameter:

| Parameter | Estimate   | Std. Error | 95% CI Lower | 95% CI Upper |
|-----------|------------|------------|---------------|-------------|
| price     | -0.0995    | 0.0063     | -0.1119       | -0.0871     |
| brand_N   |  0.9412    | 0.1187     |  0.7085       |  1.1739     |
| brand_P   |  0.5016    | 0.1215     |  0.2636       |  0.7397     |
| ad_Yes    | -0.7320    | 0.0890     | -0.9064       | -0.5576     |

### Interpretation:
Price has a statistically significant negative coefficient, confirming that higher prices reduce the probability of selection.

Brand_N and Brand_P have positive coefficients, suggesting they are preferred over the reference brand (likely Brand H).

Advertising (ad_Yes) has a negative coefficient, indicating that advertising may reduce product utility in this context.

## 5. Estimation via Bayesian Methods
::: {.callout-note title="Bayesian Posterior Summary for the 4 Parameters" collapse=true}

```{python echo=False}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Reload and reshape data
df = pd.read_csv("./conjoint_data.csv")
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)

X = df_encoded[["price", "brand_N", "brand_P", "ad_Yes"]].values
y = df_encoded["choice"].values

X_grouped = X.reshape((-1, 3, X.shape[1]))
y_grouped = y.reshape((-1, 3))
X_grouped = np.asarray(X_grouped, dtype=np.float64)
y_grouped = np.asarray(y_grouped, dtype=np.float64)

# Log-likelihood function
def log_likelihood(beta):
    utilities = np.dot(X_grouped, beta)
    exp_util = np.exp(utilities)
    probs = exp_util / exp_util.sum(axis=1, keepdims=True)
    chosen_probs = (probs * y_grouped).sum(axis=1)
    return np.log(chosen_probs + 1e-12).sum()

# Log-prior: N(0,1) for price; N(0,5) for others
def log_prior(beta):
    return -0.5 * (beta[0]**2 / 1**2 + np.sum(beta[1:]**2 / 5**2))

# Log posterior
def log_posterior(beta):
    return log_likelihood(beta) + log_prior(beta)

# MCMC settings
n_iter = 11000
burn_in = 1000
beta_dim = 4
samples = np.zeros((n_iter, beta_dim))
current_beta = np.zeros(beta_dim)
current_log_post = log_posterior(current_beta)

# Proposal distribution SDs: [0.005, 0.05, 0.05, 0.05]
proposal_sd = np.array([0.005, 0.05, 0.05, 0.05])

# MCMC sampling
np.random.seed(0)
for i in range(n_iter):
    proposal = current_beta + np.random.normal(0, proposal_sd)
    proposal_log_post = log_posterior(proposal)
    accept_prob = np.exp(proposal_log_post - current_log_post)
    if np.random.rand() < accept_prob:
        current_beta = proposal
        current_log_post = proposal_log_post
    samples[i, :] = current_beta

# Remove burn-in
samples_post = samples[burn_in:]

# Summary statistics
posterior_df = pd.DataFrame(samples_post, columns=["price", "brand_N", "brand_P", "ad_Yes"])
summary_df = posterior_df.describe(percentiles=[0.025, 0.975]).T[["mean", "std"]]
summary_df["CI Lower"] = posterior_df.quantile(0.025)
summary_df["CI Upper"] = posterior_df.quantile(0.975)

summary_df
```
:::

### Bayesian Posterior Summary for the 4 Parameters

| Parameter | Posterior Mean | Std. Dev | 95% Credible Interval Lower | 95% Credible Interval Upper |
|-----------|----------------|----------|-----------------------------|-----------------------------|
| price     | -0.0998        | 0.0065   | -0.1130                     | -0.0872                     |
| brand_N   |  0.9382        | 0.1101   |  0.7246                     |  1.1603                     |
| brand_P   |  0.4997        | 0.1076   |  0.2974                     |  0.7273                     |
| ad_Yes    | -0.7274        | 0.0904   | -0.9025                     | -0.5410                     |

### Posterior Distributions and Trace Plots

The following figure shows the trace plots and posterior histograms for each of the four parameters estimated via Metropolis-Hastings:

```{python echo=False, fig-width=10, fig-height=8}
fig, axes = plt.subplots(4, 2, figsize=(8, 8))
param_names = ["price", "brand_N", "brand_P", "ad_Yes"]

for i, param in enumerate(param_names):
    axes[i, 0].plot(posterior_df[param])
    axes[i, 0].set_title(f"Trace Plot: {param}")
    axes[i, 0].set_xlabel("Iteration")
    axes[i, 0].set_ylabel("Value")

    axes[i, 1].hist(posterior_df[param], bins=30, edgecolor='black')
    axes[i, 1].set_title(f"Posterior Histogram: {param}")
    axes[i, 1].set_xlabel("Value")
    axes[i, 1].set_ylabel("Frequency")

plt.tight_layout()
```

### MLE Vs Bayesian Estimates
```{python echo=False}
bayes_values = {
    "price": [-0.0998, 0.0065],
    "brand_N": [0.9382, 0.1101],
    "brand_P": [0.4997, 0.1076],
    "ad_Yes": [-0.7274, 0.0904]
}

compare_df = pd.DataFrame({
    "Parameter": ["price", "brand_N", "brand_P", "ad_Yes"],
    "MLE Estimate": [-0.0995, 0.9412, 0.5016, -0.7320],
    "MLE Std. Error": [0.0063, 0.1187, 0.1215, 0.0890],
    "Bayes Mean": [-0.0998, 0.9382, 0.4997, -0.7274],
    "Bayes Std. Dev": [0.0065, 0.1101, 0.1076, 0.0904]
})

compare_df
```
### Comparison of MLE and Bayesian Estimation Results

Both the Maximum Likelihood Estimation (MLE) and Bayesian methods provide consistent insights about the effects of the product features on choice behavior. Below is a comparison of their outputs:

- **Parameter Estimates**: The posterior means from the Bayesian estimation are very close to the MLE point estimates across all four parameters. For example, the coefficient on `price` is approximately -0.0998 in both methods, indicating a consistent negative effect of price on choice probability.

- **Uncertainty Quantification**:
  - MLE provides **standard errors** and **95% confidence intervals** based on the inverse Hessian.
  - Bayesian estimation yields **posterior standard deviations** and **95% credible intervals** from the posterior sample distribution.
  - Notably, the width and range of the Bayesian credible intervals are slightly more flexible, especially under weakly informative priors.

- **Regularization Effect of Priors**: 
  - The Bayesian method includes prior distributions (`N(0,1)` for `price` and `N(0,5)` for binary variables), which can slightly "shrink" estimates compared to MLE, especially in small samples. However, in this case, the priors are relatively weak, so their influence is modest.

- **Interpretability**: 
  - MLE intervals are interpreted in terms of repeated sampling (frequentist), whereas Bayesian intervals can be directly interpreted as probabilities (e.g., "There is a 95% chance the parameter lies in this interval").

In conclusion, both estimation techniques yield very similar results in this well-specified, clean dataset. The Bayesian framework provides a richer view by revealing the full posterior distribution, while MLE is computationally faster and commonly used for large datasets.


## 6. Discussion

### Interpreting the Parameter Estimates

Even though we did not simulate the data ourselves, we can still interpret the meaning of the estimated parameters:

- The **negative coefficient on price** (`β_price < 0`) makes intuitive sense: as price increases, the utility of a product decreases, making consumers less likely to choose it. This aligns with standard economic theory.

- The fact that **β_Netflix > β_Prime** (i.e., the coefficient on the `brand_N` dummy is larger than that on `brand_P`) suggests that, on average, respondents prefer the Netflix-branded product over the Prime-branded one, *all else equal*. This may be due to brand loyalty, perceived quality, or other unobserved factors favoring Netflix.

- Similarly, if **brand_Hulu is the omitted category**, then both `brand_N` and `brand_P` are being compared relative to Hulu. So we can infer a ranking of brand preferences from the sign and magnitude of the coefficients.

### Toward a Hierarchical (Random Coefficient) Model

The current multinomial logit (MNL) model assumes that **all consumers share the same coefficients** (i.e., preferences). However, in the real world, consumers are heterogeneous — for example, some are more price-sensitive than others, or may strongly prefer one brand over another.

To capture this **individual-level variation**, we can move to a **hierarchical Bayesian model** (also called a mixed logit or random coefficient model):

- **Instead of estimating a single β vector**, we assume each consumer `i` has their own vector `β_i`, drawn from a common population distribution (e.g., multivariate normal).
  
- The model becomes two-layered:
  - **Level 1 (individual):** choice probabilities are modeled as in the MNL, but using `β_i`.
  - **Level 2 (population):** we place priors on the distribution of `β_i` (e.g., mean and covariance matrix), and estimate those hyperparameters from the data.

- Technically, this would require:
  - Modifying the likelihood function to sum/integrate over the distribution of random coefficients,
  - Using more advanced sampling methods like Gibbs sampling or Hamiltonian Monte Carlo to perform posterior inference.

Such models are widely used in practice, especially in marketing research, to generate **individual-level predictions**, **segment consumers**, and **simulate responses to new product designs**.








