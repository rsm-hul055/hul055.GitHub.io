[
  {
    "objectID": "blog/first-blog/index.html",
    "href": "blog/first-blog/index.html",
    "title": "first-blog",
    "section": "",
    "text": "This is my first blog post. I’m excited to share my work with the world!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "first-blog\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hui’s Website",
    "section": "",
    "text": "Here is a paragraph about me! I am a graduate student in UCSD Rady school\nlast updated 2025-04-06"
  },
  {
    "objectID": "Resume/resume.html",
    "href": "Resume/resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "resume/resume.html",
    "href": "resume/resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Homework",
    "section": "",
    "text": "Homework 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "About\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResume\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHui’s Website\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Homework\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfirst-blog\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/projects.html",
    "href": "project/projects.html",
    "title": "My Homework",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nHui Liu\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/hw1/index.html",
    "href": "project/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe core hypothesis explored in the experiment is whether offering matching donations increases the likelihood and size of charitable contributions. Matching donations are often used by nonprofits under the belief that people are more likely to give when they know their donation will be “matched” by another funder.\nTo test this idea, Karlan and List conducted a large-scale natural field experiment involving 50,083 individuals who had previously donated to a politically conservative nonprofit organization. These individuals were randomly assigned to receive fundraising letters with different conditions: - Control group: received a standard donation request. - Treatment group: received one of three types of matching grants — a 1:1, 2:1, or 3:1 match. - Each letter also varied the match threshold (i.e., how much the funder was willing to match in total) and the suggested donation amount (either equal to or a multiplier of the donor’s previous contribution).\nThe strength of this design lies in its scale, natural setting (real donors, real money), and random assignment, which allows for clean causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "project/hw1/index.html#introduction",
    "href": "project/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe core hypothesis explored in the experiment is whether offering matching donations increases the likelihood and size of charitable contributions. Matching donations are often used by nonprofits under the belief that people are more likely to give when they know their donation will be “matched” by another funder.\nTo test this idea, Karlan and List conducted a large-scale natural field experiment involving 50,083 individuals who had previously donated to a politically conservative nonprofit organization. These individuals were randomly assigned to receive fundraising letters with different conditions: - Control group: received a standard donation request. - Treatment group: received one of three types of matching grants — a 1:1, 2:1, or 3:1 match. - Each letter also varied the match threshold (i.e., how much the funder was willing to match in total) and the suggested donation amount (either equal to or a multiplier of the donor’s previous contribution).\nThe strength of this design lies in its scale, natural setting (real donors, real money), and random assignment, which allows for clean causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "project/hw1/index.html#data",
    "href": "project/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe analyze the dataset provided by the authors, which contains 50,083 observations — each representing a recipient of a fundraising letter. The dataset includes variables describing:\n\nExperimental assignment: whether the individual received a control or treatment letter, and what kind of match ratio they received\nPast donation behavior: frequency, amount, time since last donation, etc.\nDemographics and political geography\nThe outcome variables: whether the individual donated and how much they gave\n\nBelow is the code used to load the dataset into Python and show a snapshot of its structure:\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Display shape and variable names\ndf.shape, df.columns.tolist()\n\n((50083, 51),\n ['treatment',\n  'control',\n  'ratio',\n  'ratio2',\n  'ratio3',\n  'size',\n  'size25',\n  'size50',\n  'size100',\n  'sizeno',\n  'ask',\n  'askd1',\n  'askd2',\n  'askd3',\n  'ask1',\n  'ask2',\n  'ask3',\n  'amount',\n  'gave',\n  'amountchange',\n  'hpa',\n  'ltmedmra',\n  'freq',\n  'years',\n  'year5',\n  'mrm2',\n  'dormant',\n  'female',\n  'couple',\n  'state50one',\n  'nonlit',\n  'cases',\n  'statecnt',\n  'stateresponse',\n  'stateresponset',\n  'stateresponsec',\n  'stateresponsetminc',\n  'perbush',\n  'close25',\n  'red0',\n  'blue0',\n  'redcty',\n  'bluecty',\n  'pwhite',\n  'pblack',\n  'page18_39',\n  'ave_hh_sz',\n  'median_hhincome',\n  'powner',\n  'psch_atlstba',\n  'pop_propurban'])\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWe begin by checking whether the treatment and control groups are balanced in terms of observable characteristics. This is a common first step in analyzing randomized experiments: if randomization was implemented correctly, both groups should be similar on all baseline variables.\nHere, we focus on one such variable: the number of months since a donor’s last contribution (mrm2).\nThe average for the control group is 12.99 months, while for the treatment group it is 13.01 months. A t-test comparing these means yields t = 0.12, p = 0.905, indicating no statistically significant difference.\nWe also estimate a linear regression of mrm2 on the treatment indicator. The coefficient is effectively zero and not statistically significant, confirming the same conclusion.\nThese results suggest that the randomization was successful: the two groups appear well-balanced with respect to prior giving behavior. This gives us confidence that subsequent differences in donation outcomes can be attributed to the experimental treatments.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n#  treatment + control + mrm2\ndf_subset = df[[\"treatment\", \"control\", \"mrm2\"]].dropna()\n\n# \nmeans = df_subset.groupby(\"treatment\")[\"mrm2\"].mean()\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df_subset[df_subset[\"treatment\"] == 1][\"mrm2\"],\n    df_subset[df_subset[\"treatment\"] == 0][\"mrm2\"],\n    equal_var=False  # Welch's t-test\n)\n\n\nX = sm.add_constant(df_subset[\"treatment\"])\nmodel = sm.OLS(df_subset[\"mrm2\"], X).fit()\n\n\nprint(\"=== mean value ===\")\nprint(means)\nprint(\"\\n=== t-test ===\")\nprint(f\"t = {t_stat:.3f}, p = {p_value:.3f}\")\nprint(\"\\n=== OLS Regression result ===\")\nprint(model.summary())\n\n=== mean value ===\ntreatment\n0    12.998142\n1    13.011828\nName: mrm2, dtype: float64\n\n=== t-test ===\nt = 0.120, p = 0.905\n\n=== OLS Regression result ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\nTime:                        23:17:50   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\nWhy check for balance?\n\n\n\nBalance tests help confirm that the randomization worked as expected. If the treatment and control groups are similar on baseline characteristics, we can be more confident that later differences in donation outcomes are due to the treatment and not underlying differences between groups."
  },
  {
    "objectID": "project/hw1/index.html#balance-check",
    "href": "project/hw1/index.html#balance-check",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Check",
    "text": "Balance Check\nWe begin by checking whether the treatment and control groups are balanced in terms of observable characteristics. This is a common first step in analyzing randomized experiments: if randomization was implemented correctly, both groups should be similar on all baseline variables.\nHere, we focus on one such variable: the number of months since a donor’s last contribution (mrm2).\nThe average for the control group is 12.99 months, while for the treatment group it is 13.01 months. A t-test comparing these means yields t = 0.12, p = 0.905, indicating no statistically significant difference.\nWe also estimate a linear regression of mrm2 on the treatment indicator. The coefficient is effectively zero and not statistically significant, confirming the same conclusion.\nThese results suggest that the randomization was successful: the two groups appear well-balanced with respect to prior giving behavior. This gives us confidence that subsequent differences in donation outcomes can be attributed to the experimental treatments.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n#  treatment + control + mrm2\ndf_subset = df[[\"treatment\", \"control\", \"mrm2\"]].dropna()\n\n# \nmeans = df_subset.groupby(\"treatment\")[\"mrm2\"].mean()\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df_subset[df_subset[\"treatment\"] == 1][\"mrm2\"],\n    df_subset[df_subset[\"treatment\"] == 0][\"mrm2\"],\n    equal_var=False  # Welch's t-test\n)\n\n\nX = sm.add_constant(df_subset[\"treatment\"])\nmodel = sm.OLS(df_subset[\"mrm2\"], X).fit()\n\n\nprint(\"=== mean value ===\")\nprint(means)\nprint(\"\\n=== t-test ===\")\nprint(f\"t = {t_stat:.3f}, p = {p_value:.3f}\")\nprint(\"\\n=== OLS Regression result ===\")\nprint(model.summary())\n\n=== mean value ===\ntreatment\n0    12.998142\n1    13.011828\nName: mrm2, dtype: float64\n\n=== t-test ===\nt = 0.120, p = 0.905\n\n=== OLS Regression result ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\nTime:                        22:53:57   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\nWhy check for balance?\n\n\n\nBalance tests help confirm that the randomization worked as expected. If the treatment and control groups are similar on baseline characteristics, we can be more confident that later differences in donation outcomes are due to the treatment and not underlying differences between groups."
  },
  {
    "objectID": "project/hw1/index.html#experimental-results",
    "href": "project/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe begin by comparing donation rates between the treatment and control groups. In the graph below, we see the proportion of individuals who gave any donation, split by whether they received a matching offer or not.\n\nimport statsmodels.formula.api as smf\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df[df[\"treatment\"] == 1][\"gave\"],\n    df[df[\"treatment\"] == 0][\"gave\"],\n    equal_var=False\n)\n\n# linear regression\nols_model = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Probit \nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\n\n\nfig, ax = plt.subplots(figsize=(6,4))\ngave_by_group.plot(kind='bar', color=[\"pink\", \"blue\"], ax=ax)\nax.set_ylabel(\"Proportion Donated\")\nax.set_title(\"Donation Rate by Treatment Group\")\nax.set_xticklabels([\"Control\", \"Treatment\"], rotation=0)\nax.set_ylim(0, 0.05)\nplt.tight_layout()\n\n\nprint(\"t =\", t_stat, \"p =\", p_value)\n\nprint(ols_model.summary2().tables[1])\n\nprint(probit_model.summary2().tables[1])\n\nt = 3.2094621908279835 p = 0.001330982345091417\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  0.017858  0.001101  16.224643  4.779032e-59  0.015701  0.020016\ntreatment  0.004180  0.001348   3.101361  1.927403e-03  0.001538  0.006822\n              Coef.  Std.Err.         z     P&gt;|z|    [0.025    0.975]\nIntercept -2.100141  0.023316 -90.07277  0.000000 -2.145840 -2.054443\ntreatment  0.086785  0.027879   3.11293  0.001852  0.032143  0.141426\n\n\n\n\n\n\n\n\n\nThe donation rate for the control group is approximately 1.8%, while for the treatment group it is 2.2%. This difference is statistically significant:\nA t-test yields t = 3.21, p = 0.0013, indicating that the observed difference is unlikely to be due to random chance.\nA linear probability model (OLS) confirms this result with a positive and significant coefficient on treatment:\n\nols_model.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.017858\n0.001101\n16.224643\n4.779032e-59\n0.015701\n0.020016\n\n\ntreatment\n0.004180\n0.001348\n3.101361\n1.927403e-03\n0.001538\n0.006822\n\n\n\n\n\n\n\nTo further validate the result, we estimate a probit model of donation on treatment status. The probit coefficient is 0.087 (p = 0.0019), again confirming a statistically significant effect of matching offers.\n\nprobit_model.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-2.100141\n0.023316\n-90.07277\n0.000000\n-2.145840\n-2.054443\n\n\ntreatment\n0.086785\n0.027879\n3.11293\n0.001852\n0.032143\n0.141426\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nThese results suggest that offering a matching donation significantly increases the likelihood that a person will donate. From a behavioral standpoint, this provides evidence that people are more motivated to give when they feel their contribution will be amplified.\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nT-test Comparisons To examine whether the size of the match ratio affects donation behavior, I restrict the sample to the treatment group and compute the donation rate for each ratio subgroup:\n1:1 match ratio → 2.07%\n2:1 match ratio → 2.02%\n3:1 match ratio → 2.27%\nI then perform t-tests to compare the donation rates between different match sizes:\n1:1 vs 2:1: p = 0.3345\n2:1 vs 3:1: p = 0.9600\nThese p-values indicate that the differences in response rates across match ratios are not statistically significant. Therefore, larger match ratios do not appear to generate significantly higher giving rates, consistent with what Karlan & List describe on page 8: “the figures suggest no consistent pattern.”\nRegression Analysis To validate this further, I estimate a linear regression model with dummy variables indicating the different match ratios:\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf[\"ratio1\"] = (df[\"ratio\"] == 1).astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == 2).astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == 3).astype(int)\n\n\ndf_matched = df[df[\"treatment\"] == 1]\n\n\nX = df_matched[[\"ratio1\", \"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df_matched[\"gave\"]\n\nmodel = sm.OLS(y, X).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n-1.597075e+09\n3.428594e+11\n-0.004658\n0.996283\n-6.736135e+11\n6.704193e+11\n\n\nratio1\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio2\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio3\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\n\n\n\n\n\nThe regression on gave using these dummies among the treatment group yields:\nAll coefficients are not statistically significant (p &gt; 0.99)\nCoefficients are extremely small, confirming the weak impact of match ratio levels on response behavior\nDirect Comparison of Response Rates Finally, I directly compute the differences in average donation rates:\n2:1 – 1:1 difference = 0.0019\n3:1 – 2:1 difference = 0.0001\nAgain, the numerical differences are very small, further supporting the conclusion that increasing the match ratio from 1:1 to 3:1 does not lead to materially greater giving.\n\n\n\n\n\n\nConclusion\n\n\n\nTaken together, these results replicate the original findings: while matched donations do increase overall giving compared to no match, increasing the generosity of the match offer (from 1:1 to 3:1) does not significantly boost response rates.\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nFull Sample We begin by regressing the donation amount on the treatment status across the full sample. The estimated coefficient on the treatment dummy is 0.154, but the p-value is approximately 0.063, which suggests the result is not statistically significant at the 5% level. This implies that when considering all individuals, offering a matching grant may increase donation amounts, but the evidence is not conclusive.\nConditional on Donating Next, we restrict the sample to only those who made a donation. This allows us to analyze how much respondents donate conditional on giving something. The treatment coefficient becomes -1.669, with a p-value of 0.561, indicating no statistically significant difference in the donation size between treatment and control groups among those who chose to give.\nThis suggests that while the offer of a match may increase the likelihood of donating (as shown earlier), it does not significantly impact the amount donated by those who do choose to donate.\nDistributional Plots We also visualize the distribution of donation amounts for treatment and control groups using histograms:\n\nimport matplotlib.pyplot as plt\n\n\ncontrol_amounts = df[(df[\"treatment\"] == 0) & (df[\"gave\"] == 1)][\"amount\"]\ntreatment_amounts = df[(df[\"treatment\"] == 1) & (df[\"gave\"] == 1)][\"amount\"]\n\n# Plot for control group\nfig, ax = plt.subplots()\nax.hist(control_amounts, bins=30, alpha=0.7, label=\"Control\", color=\"skyblue\")\nax.axvline(control_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Control Mean\")\nax.set_title(\"Donation Amounts: Control Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig\n\n# Plot for treatment group\nfig, ax = plt.subplots()\nax.hist(treatment_amounts, bins=30, alpha=0.7, label=\"Treatment\", color=\"lightgreen\")\nax.axvline(treatment_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Treatment Mean\")\nax.set_title(\"Donation Amounts: Treatment Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig"
  },
  {
    "objectID": "project/hw1/index.html#simulation-experiment",
    "href": "project/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThis chart illustrates the Law of Large Numbers using a simulated experiment. I generated 10,000 simulated donation outcomes for the control group (Bernoulli p = 0.018) and 10,000 for the treatment group (Bernoulli p = 0.022), and then computed the cumulative average of their differences.\nAs shown, the cumulative average is quite noisy at the beginning due to the small sample size — early differences fluctuate dramatically. However, as the number of observations increases, the cumulative average stabilizes around 0.004 (the true treatment effect), demonstrating convergence.\nThis supports the intuition behind large sample inference: as the sample size grows, random variation diminishes, and estimates converge to their true values.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\np_control = 0.018\np_treatment = 0.022\nn = 10_000\n\n\nnp.random.seed(42)\ncontrol = np.random.binomial(n=1, p=p_control, size=n)\ntreatment = np.random.binomial(n=1, p=p_treatment, size=n)\n\n\ndiff = treatment - control\ncumulative_avg = np.cumsum(diff) / np.arange(1, n + 1)\n\n# Plot \nfig, ax = plt.subplots()\nax.plot(cumulative_avg, label=\"Cumulative Average\")\nax.axhline(y=0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nax.set_title(\"Law of Large Numbers: Cumulative Average of Differences\")\nax.set_xlabel(\"Sample Size\")\nax.set_ylabel(\"Cumulative Average\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(123)\n\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\n\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 4))\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(1000):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treatment, n)\n        diff = treatment.mean() - control.mean()\n        differences.append(diff)\n\n    ax = axs[i // 2, i % 2]\n    ax.hist(differences, bins=30, color=\"grey\", edgecolor=\"black\")\n    ax.set_title(f\"Sample Size: {n}\")\n    ax.set_xlabel(\"Average Difference\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown above, these histograms display the distribution of average differences across 1000 simulations for various sample sizes. When the sample size is small (e.g., 50), the distribution is highly variable and not symmetric. As the sample size increases, the distributions become increasingly symmetric and bell-shaped, illustrating the Central Limit Theorem: the sampling distribution of the sample mean approaches a normal distribution as the sample size increases."
  },
  {
    "objectID": "project/hw1/hw1.html",
    "href": "project/hw1/hw1.html",
    "title": "HuiL's Website",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n#  treatment + control + mrm2\ndf_subset = df[[\"treatment\", \"control\", \"mrm2\"]].dropna()\n\n# \nmeans = df_subset.groupby(\"treatment\")[\"mrm2\"].mean()\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df_subset[df_subset[\"treatment\"] == 1][\"mrm2\"],\n    df_subset[df_subset[\"treatment\"] == 0][\"mrm2\"],\n    equal_var=False  # Welch's t-test\n)\n\n\nX = sm.add_constant(df_subset[\"treatment\"])\nmodel = sm.OLS(df_subset[\"mrm2\"], X).fit()\n\n\nprint(\"=== mean value ===\")\nprint(means)\nprint(\"\\n=== t-test ===\")\nprint(f\"t = {t_stat:.3f}, p = {p_value:.3f}\")\nprint(\"\\n=== OLS Regression result ===\")\nprint(model.summary())\n\n=== mean value ===\ntreatment\n0    12.998142\n1    13.011828\nName: mrm2, dtype: float64\n\n=== t-test ===\nt = 0.120, p = 0.905\n\n=== OLS Regression result ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\nTime:                        21:28:17   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nimport statsmodels.formula.api as smf\n\n\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df[df[\"treatment\"] == 1][\"gave\"],\n    df[df[\"treatment\"] == 0][\"gave\"],\n    equal_var=False\n)\n\n\nols_model = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Probit \nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\n\n\nfig, ax = plt.subplots(figsize=(6,4))\ngave_by_group.plot(kind='bar', color=[\"pink\", \"blue\"], ax=ax)\nax.set_ylabel(\"Proportion Donated\")\nax.set_title(\"Donation Rate by Treatment Group\")\nax.set_xticklabels([\"Control\", \"Treatment\"], rotation=0)\nax.set_ylim(0, 0.05)\nplt.tight_layout()\n\n\nprint(\"t =\", t_stat, \"p =\", p_value)\n\nprint(ols_model.summary2().tables[1])\n\nprint(probit_model.summary2().tables[1])\n\nt = 3.2094621908279835 p = 0.001330982345091417\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  0.017858  0.001101  16.224643  4.779032e-59  0.015701  0.020016\ntreatment  0.004180  0.001348   3.101361  1.927403e-03  0.001538  0.006822\n              Coef.  Std.Err.         z     P&gt;|z|    [0.025    0.975]\nIntercept -2.100141  0.023316 -90.07277  0.000000 -2.145840 -2.054443\ntreatment  0.086785  0.027879   3.11293  0.001852  0.032143  0.141426\n\n\n\n\n\n\n\n\n\n\n\ndf_matched = df[(df[\"treatment\"] == 1)]\n\n\nrates = df_matched.groupby(\"ratio\")[\"gave\"].mean()\nprint(\"Donation rates by match ratio:\")\nprint(rates)\n\n# t-tests\nfrom scipy.stats import ttest_ind\n\n# 1:1 vs 2:1\np12 = ttest_ind(df_matched[df_matched[\"ratio\"] == 1][\"gave\"],\n                df_matched[df_matched[\"ratio\"] == 2][\"gave\"],\n                equal_var=False)\n\n# 2:1 vs 3:1\np23 = ttest_ind(df_matched[df_matched[\"ratio\"] == 2][\"gave\"],\n                df_matched[df_matched[\"ratio\"] == 3][\"gave\"],\n                equal_var=False)\n\nprint(f\"1:1 vs 2:1 t-test p = {p12.pvalue:.4f}\")\nprint(f\"2:1 vs 3:1 t-test p = {p23.pvalue:.4f}\")\n\nDonation rates by match ratio:\nratio\nControl         NaN\n1          0.020749\n2          0.022633\n3          0.022733\nName: gave, dtype: float64\n1:1 vs 2:1 t-test p = 0.3345\n2:1 vs 3:1 t-test p = 0.9600\n\n\n/tmp/ipykernel_2616514/617877570.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  rates = df_matched.groupby(\"ratio\")[\"gave\"].mean()\n\n\n\n#\ndf[\"ratio1\"] = (df[\"ratio\"] == 1).astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == 2).astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == 3).astype(int)\n\n\ndf_matched = df[df[\"treatment\"] == 1]\n\n# 回归\nimport statsmodels.api as sm\n\nX = df_matched[[\"ratio1\", \"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df_matched[\"gave\"]\n\nmodel = sm.OLS(y, X).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n-1.597075e+09\n3.428594e+11\n-0.004658\n0.996283\n-6.736135e+11\n6.704193e+11\n\n\nratio1\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio2\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio3\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\n\n\n\n\n\n\n# 响应率\nrate_1 = df_matched[df_matched[\"ratio\"] == 1][\"gave\"].mean()\nrate_2 = df_matched[df_matched[\"ratio\"] == 2][\"gave\"].mean()\nrate_3 = df_matched[df_matched[\"ratio\"] == 3][\"gave\"].mean()\n\nprint(f\"1:1 vs 2:1 difference: {rate_2 - rate_1:.4f}\")\nprint(f\"2:1 vs 3:1 difference: {rate_3 - rate_2:.4f}\")\n\n1:1 vs 2:1 difference: 0.0019\n2:1 vs 3:1 difference: 0.0001\n\n\n\nimport statsmodels.api as sm\n\nX = df[\"treatment\"]\nX = sm.add_constant(X)\ny = df[\"amount\"]\n\nmodel = sm.OLS(y, X).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n0.813268\n0.067418\n12.062995\n1.843438e-33\n0.681127\n0.945409\n\n\ntreatment\n0.153605\n0.082561\n1.860503\n6.282029e-02\n-0.008216\n0.315426\n\n\n\n\n\n\n\n\ndf_gave = df[df[\"gave\"] == 1]\n\nX = df_gave[\"treatment\"]\nX = sm.add_constant(X)\ny = df_gave[\"amount\"]\n\nmodel_gave = sm.OLS(y, X).fit()\nmodel_gave.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n45.540268\n2.423378\n18.792063\n5.473578e-68\n40.784958\n50.295579\n\n\ntreatment\n-1.668393\n2.872384\n-0.580839\n5.614756e-01\n-7.304773\n3.967986\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Prepare data\ncontrol_amounts = df[(df[\"treatment\"] == 0) & (df[\"gave\"] == 1)][\"amount\"]\ntreatment_amounts = df[(df[\"treatment\"] == 1) & (df[\"gave\"] == 1)][\"amount\"]\n\n# Plot for control group\nfig, ax = plt.subplots()\nax.hist(control_amounts, bins=30, alpha=0.7, label=\"Control\", color=\"skyblue\")\nax.axvline(control_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Control Mean\")\nax.set_title(\"Donation Amounts: Control Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig\n\n# Plot for treatment group\nfig, ax = plt.subplots()\nax.hist(treatment_amounts, bins=30, alpha=0.7, label=\"Treatment\", color=\"lightgreen\")\nax.axvline(treatment_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Treatment Mean\")\nax.set_title(\"Donation Amounts: Treatment Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set probabilities\np_control = 0.018\np_treatment = 0.022\nn = 10_000\n\n# Simulate donation outcomes\nnp.random.seed(42)\ncontrol = np.random.binomial(n=1, p=p_control, size=n)\ntreatment = np.random.binomial(n=1, p=p_treatment, size=n)\n\n# Calculate difference vector and cumulative average\ndiff = treatment - control\ncumulative_avg = np.cumsum(diff) / np.arange(1, n + 1)\n\n# Plot cumulative average\nfig, ax = plt.subplots()\nax.plot(cumulative_avg, label=\"Cumulative Average\")\nax.axhline(y=0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nax.set_title(\"Law of Large Numbers: Cumulative Average of Differences\")\nax.set_xlabel(\"Sample Size\")\nax.set_ylabel(\"Cumulative Average\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed\nnp.random.seed(123)\n\n# Simulation parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\n\n# Create 4 histograms\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(1000):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treatment, n)\n        diff = treatment.mean() - control.mean()\n        differences.append(diff)\n\n    ax = axs[i // 2, i % 2]\n    ax.hist(differences, bins=30, color=\"grey\", edgecolor=\"black\")\n    ax.set_title(f\"Sample Size: {n}\")\n    ax.set_xlabel(\"Average Difference\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nfig"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. For at least one variable, perform the test as both t-test (use the formula in the class slides) and separately as a linear regression (regress for example mrm2 on treatment); confirm both methods yield the exact same results. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made (you can do this as a bivariate linear regression if you want). It may help to confirm your calculations match Table 2a Panel A. Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or something that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. This average will likely be “noisey” when only averaging a few numbers, but should “settle down” and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms at sample sizes 50, 200, 500, and 1000. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader."
  },
  {
    "objectID": "project/hw1/index.html#charitable-contribution-made-1",
    "href": "project/hw1/index.html#charitable-contribution-made-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Charitable Contribution Made",
    "text": "Charitable Contribution Made\nWe begin by comparing donation rates between the treatment and control groups. In the graph below, we see the proportion of individuals who gave any donation, split by whether they received a matching offer or not.\n\nimport statsmodels.formula.api as smf\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n# 创建 treatment vs control 捐款比例 barplot 所需数据\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\n# t-test 检验：是否捐款 (gave) 在 treatment vs control 中有显著差异\nt_stat, p_value = stats.ttest_ind(\n    df[df[\"treatment\"] == 1][\"gave\"],\n    df[df[\"treatment\"] == 0][\"gave\"],\n    equal_var=False\n)\n\n# 线性回归：gave ~ treatment\nols_model = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Probit 回归\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\n\n# 绘图\nfig, ax = plt.subplots(figsize=(6,4))\ngave_by_group.plot(kind='bar', color=[\"pink\", \"blue\"], ax=ax)\nax.set_ylabel(\"Proportion Donated\")\nax.set_title(\"Donation Rate by Treatment Group\")\nax.set_xticklabels([\"Control\", \"Treatment\"], rotation=0)\nax.set_ylim(0, 0.05)\nplt.tight_layout()\n\n\nprint(\"t =\", t_stat, \"p =\", p_value)\n\nprint(ols_model.summary2().tables[1])\n\nprint(probit_model.summary2().tables[1])\n\nt = 3.2094621908279835 p = 0.001330982345091417\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  0.017858  0.001101  16.224643  4.779032e-59  0.015701  0.020016\ntreatment  0.004180  0.001348   3.101361  1.927403e-03  0.001538  0.006822\n              Coef.  Std.Err.         z     P&gt;|z|    [0.025    0.975]\nIntercept -2.100141  0.023316 -90.07277  0.000000 -2.145840 -2.054443\ntreatment  0.086785  0.027879   3.11293  0.001852  0.032143  0.141426\n\n\n\n\n\n\n\n\n\nThe donation rate for the control group is approximately 1.8%, while for the treatment group it is 2.2%. This difference is statistically significant:\nA t-test yields t = 3.21, p = 0.0013, indicating that the observed difference is unlikely to be due to random chance.\nA linear probability model (OLS) confirms this result with a positive and significant coefficient on treatment:\n\nols_model.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.017858\n0.001101\n16.224643\n4.779032e-59\n0.015701\n0.020016\n\n\ntreatment\n0.004180\n0.001348\n3.101361\n1.927403e-03\n0.001538\n0.006822\n\n\n\n\n\n\n\nTo further validate the result, we estimate a probit model of donation on treatment status. The probit coefficient is 0.087 (p = 0.0019), again confirming a statistically significant effect of matching offers.\n\nprobit_model.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-2.100141\n0.023316\n-90.07277\n0.000000\n-2.145840\n-2.054443\n\n\ntreatment\n0.086785\n0.027879\n3.11293\n0.001852\n0.032143\n0.141426\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nThese results suggest that offering a matching donation significantly increases the likelihood that a person will donate. From a behavioral standpoint, this provides evidence that people are more motivated to give when they feel their contribution will be amplified.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nT-test Comparisons To examine whether the size of the match ratio affects donation behavior, I restrict the sample to the treatment group and compute the donation rate for each ratio subgroup:\n1:1 match ratio → 2.07%\n2:1 match ratio → 2.02%\n3:1 match ratio → 2.27%\nI then perform t-tests to compare the donation rates between different match sizes:\n1:1 vs 2:1: p = 0.3345\n2:1 vs 3:1: p = 0.9600\nThese p-values indicate that the differences in response rates across match ratios are not statistically significant. Therefore, larger match ratios do not appear to generate significantly higher giving rates, consistent with what Karlan & List describe on page 8: “the figures suggest no consistent pattern.”\nRegression Analysis To validate this further, I estimate a linear regression model with dummy variables indicating the different match ratios:\n\ndf[\"ratio1\"] = (df[\"ratio\"] == 1).astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == 2).astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == 3).astype(int)\n\nThe regression on gave using these dummies among the treatment group yields:\nAll coefficients are not statistically significant (p &gt; 0.99)\nCoefficients are extremely small, confirming the weak impact of match ratio levels on response behavior\nDirect Comparison of Response Rates Finally, I directly compute the differences in average donation rates:\n2:1 – 1:1 difference = 0.0019\n3:1 – 2:1 difference = 0.0001\nAgain, the numerical differences are very small, further supporting the conclusion that increasing the match ratio from 1:1 to 3:1 does not lead to materially greater giving.\n\n\n\n\n\n\nConclusion\n\n\n\nTaken together, these results replicate the original findings: while matched donations do increase overall giving compared to no match, increasing the generosity of the match offer (from 1:1 to 3:1) does not significantly boost response rates.\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nFull Sample We begin by regressing the donation amount on the treatment status across the full sample. The estimated coefficient on the treatment dummy is 0.154, but the p-value is approximately 0.063, which suggests the result is not statistically significant at the 5% level. This implies that when considering all individuals, offering a matching grant may increase donation amounts, but the evidence is not conclusive.\nConditional on Donating Next, we restrict the sample to only those who made a donation. This allows us to analyze how much respondents donate conditional on giving something. The treatment coefficient becomes -1.669, with a p-value of 0.561, indicating no statistically significant difference in the donation size between treatment and control groups among those who chose to give.\nThis suggests that while the offer of a match may increase the likelihood of donating (as shown earlier), it does not significantly impact the amount donated by those who do choose to donate.\nDistributional Plots We also visualize the distribution of donation amounts for treatment and control groups using histograms:\n\n# Paste your existing plotting code here.\nimport matplotlib.pyplot as plt\n\n# Prepare data\ncontrol_amounts = df[(df[\"treatment\"] == 0) & (df[\"gave\"] == 1)][\"amount\"]\ntreatment_amounts = df[(df[\"treatment\"] == 1) & (df[\"gave\"] == 1)][\"amount\"]\n\n# Plot for control group\nfig, ax = plt.subplots()\nax.hist(control_amounts, bins=30, alpha=0.7, label=\"Control\", color=\"skyblue\")\nax.axvline(control_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Control Mean\")\nax.set_title(\"Donation Amounts: Control Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig\n\n# Plot for treatment group\nfig, ax = plt.subplots()\nax.hist(treatment_amounts, bins=30, alpha=0.7, label=\"Treatment\", color=\"lightgreen\")\nax.axvline(treatment_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Treatment Mean\")\nax.set_title(\"Donation Amounts: Treatment Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig"
  },
  {
    "objectID": "project/hw2/index.html",
    "href": "project/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n#| echo: true\n#| results: 'hide'\ndf.info(), df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n(None,\n    patents     region   age  iscustomer\n 0        0    Midwest  32.5           0\n 1        3  Southwest  37.5           0\n 2        4  Northwest  27.0           1\n 3        3  Northeast  24.5           0\n 4        3  Southwest  37.0           0)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nmean_patents = df.groupby(\"iscustomer\")[\"patents\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"stack\", palette=\"Set2\", bins=20)\nplt.xlabel(\"Number of Patents (Last 5 Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Distribution of Patents by Blueprinty Customer Status\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\nprint(\"Mean number of patents by customer status:\")\nprint(mean_patents.round(2))\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\niscustomer\nNon-Customers    3.47\nCustomers        4.13\nName: patents, dtype: float64\n\n\nThe chart above shows the distribution of the number of patents granted over the past five years, categorized by whether a company is a Blueprinty customer. We also calculated the average number of patents for each group:\n\nNon-customers: 3.47 patents\n\nCustomers: 4.13 patents\n\nBased on both the histogram and the means, companies that use Blueprinty’s software tend to hold more patents overall. This provides some support for the marketing team’s claim that Blueprinty customers are more successful in obtaining patents.\nHowever, this observation alone does not prove that the software causes higher success rates—because there may be systematic differences between customers and non-customers (such as company age or geographic location). Therefore, we need to further analyze these variables in the next steps.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo explore this possibility, we compared the regional distribution and firm age between the two groups.\nAs shown in the bar chart below, Blueprinty customers are not evenly distributed across regions. In particular, a notably higher proportion of customers come from the Northeast region, while other regions are more heavily populated by non-customers. This suggests that regional factors—such as industry concentration or market penetration—may influence software adoption.\nSimilarly, in terms of firm age, customers appear to be slightly older on average than non-customers (26.9 vs. 26.1 years). While the difference is modest, it indicates that older firms may be more likely to use Blueprinty’s software, potentially due to larger scale or greater administrative resources.\nThese findings support the idea that regional and demographic factors may confound any observed relationship between software use and patent success, and should be accounted for in further modeling.\n\nimport matplotlib.ticker as mtick\n\n\nplt.figure(figsize=(10, 5))\nregion_crosstab = pd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize=\"index\")\nregion_crosstab.plot(kind=\"bar\", stacked=True, color=[\"#d95f02\", \"#1b9e77\"])\nplt.title(\"Customer Distribution by Region\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Region\")\nplt.legend([\"Non-Customers\", \"Customers\"], title=\"Customer Status\")\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", multiple=\"stack\", bins=20, palette=\"Set2\")\nplt.title(\"Distribution of Firm Ages by Customer Status\")\nplt.xlabel(\"Firm Age (Years Since Incorporation)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# Mean ages\ndf.groupby(\"iscustomer\")[\"age\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n&lt;Figure size 960x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers    26.101570\nCustomers        26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe assume that the number of patents ( Y ) for each firm follows a Poisson distribution with rate parameter ( ):\n\\[\nY \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function (i.e., the likelihood for a single observation) is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nAssuming that observations are independent across firms, the likelihood for the full sample ( Y_1, Y_2, , Y_n ) is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nThis log-likelihood function is the basis for estimating the maximum likelihood value of ( ).\nThis log-likelihood function depends on both λ and the observed values Y.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, Y):\n\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nWe evaluated the Poisson log-likelihood over a grid of candidate λ values. As shown in the plot below, the log-likelihood is maximized at approximately λ = 3.68, which equals the sample mean.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nY = df[\"patents\"].values\n\n# Create a range of lambda values\nlambda_values = np.linspace(1, 7, 200)\nlog_likelihoods = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, log_likelihoods, color=\"blue\", label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color=\"red\", linestyle=\"--\", label=f\"Sample Mean = {np.mean(Y):.2f}\")\nplt.title(\"Poisson Log-Likelihood Curve\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe now take the analytical approach to derive the Maximum Likelihood Estimator (MLE) for ( ) under the Poisson model.\nRecall the log-likelihood function for independent observations ( Y_1, Y_2, , Y_n () ):\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTo find the MLE, we take the derivative of the log-likelihood with respect to ( ) and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = -n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThis result confirms that the MLE of ( ) under the Poisson distribution is simply the sample mean ( {Y} ), which is intuitive since ( [Y] = ) in the Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood (because we minimize)\ndef neg_log_likelihood(lmbda, Y):\n    return -poisson_log_likelihood(lmbda[0], Y)\n\n# Initial guess for lambda\ninitial_lambda = [2.0]\n\n# Run optimization\nresult = minimize(neg_log_likelihood, x0=initial_lambda, args=(Y,), bounds=[(1e-5, None)])\n\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846666035175017\n\n\nWe now use numerical optimization to find the value of λ that maximizes the Poisson log-likelihood. Since scipy.optimize.minimize() minimizes functions by default, we minimize the negative log-likelihood. The optimization result confirms our earlier finding: the MLE of λ is approximately equal to the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe define the following log-likelihood function for Poisson regression:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n\n    Xb = X @ beta  # linear predictor\n    lambdas = np.exp(Xb)  # inverse link\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport patsy\n\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n\n# Standardize age and age^2 (z-score, then round to 1 decimal place)\ndf[\"age_std\"] = ((df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()).round(1)\ndf[\"age_sq_std\"] = (df[\"age_std\"] ** 2).round(1)\n\n# Design matrix: intercept, standardized age, age^2, region dummies, iscustomer\nX = patsy.dmatrix(\"1 + age_std + age_sq_std + C(region, Treatment(reference='Midwest')) + iscustomer\", df, return_type=\"dataframe\")\nY = df[\"patents\"].values\nX_matrix = X.values\n\n# Define Poisson regression log-likelihood\ndef poisson_regression_loglikelihood(beta, Y, X):\n    Xb = X @ beta\n    lambdas = np.exp(Xb)\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n# Negative log-likelihood for optimization\ndef neg_loglik(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\n# Initial beta guess\ninitial_beta = np.zeros(X_matrix.shape[1])\n\n# Optimize with bounds and BFGS\nresult = minimize(neg_loglik, initial_beta, args=(Y, X_matrix), method=\"BFGS\", options={\"disp\": True})\n\n\nbeta_hat = result.x\nhessian = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian))\n\n\nsummary_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.design_info.column_names)\n\nOptimization terminated successfully.\n         Current function value: 3257.709028\n         Iterations: 18\n         Function evaluations: 243\n         Gradient evaluations: 27\n\n\nThe table below reports the estimated coefficients and standard errors from the Poisson regression model.\n\nThe coefficient on iscustomer is 0.2084 (SE = 0.0310), suggesting that Blueprinty customers are associated with a significantly higher rate of patent approvals, even after controlling for age and region.\nThe negative coefficients on both age_std and age_sq_std suggest a concave (inverted-U) relationship between firm age and patenting activity.\nRegional effects appear small and statistically insignificant relative to the Midwest baseline.\n\n\nsummary_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n1.3422\n0.0397\n\n\nC(region, Treatment(reference='Midwest'))[T.Northeast]\n0.0302\n0.0472\n\n\nC(region, Treatment(reference='Midwest'))[T.Northwest]\n-0.0179\n0.0575\n\n\nC(region, Treatment(reference='Midwest'))[T.South]\n0.0564\n0.0561\n\n\nC(region, Treatment(reference='Midwest'))[T.Southwest]\n0.0514\n0.0501\n\n\nage_std\n-0.0596\n0.0143\n\n\nage_sq_std\n-0.1549\n0.0142\n\n\niscustomer\n0.2084\n0.0310\n\n\n\n\n\n\n\nTo verify our results, we re-estimate the model using Python’s built-in GLM() function from the statsmodels package with a Poisson family.\n\nimport statsmodels.api as sm\nglm_model = sm.GLM(Y, X_matrix, family=sm.families.Poisson())\nglm_results = glm_model.fit()\nglm_summary_df = pd.DataFrame({\n    \"Coefficient (GLM)\": glm_results.params,\n    \"Std. Error (GLM)\": glm_results.bse\n}, index=X.design_info.column_names)\n\n# Summary table\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3257.7\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n2142.5\n\n\nTime:\n20:25:08\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1364\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.3422\n0.038\n35.054\n0.000\n1.267\n1.417\n\n\nx1\n0.0302\n0.044\n0.691\n0.489\n-0.055\n0.116\n\n\nx2\n-0.0179\n0.054\n-0.333\n0.739\n-0.123\n0.087\n\n\nx3\n0.0564\n0.053\n1.070\n0.285\n-0.047\n0.160\n\n\nx4\n0.0514\n0.047\n1.089\n0.276\n-0.041\n0.144\n\n\nx5\n-0.0596\n0.015\n-3.967\n0.000\n-0.089\n-0.030\n\n\nx6\n-0.1549\n0.013\n-11.515\n0.000\n-0.181\n-0.129\n\n\nx7\n0.2084\n0.031\n6.743\n0.000\n0.148\n0.269\n\n\n\n\n\nThe coefficient estimates from GLM() match those obtained via our custom maximum likelihood estimation, confirming the correctness of our implementation. Standard errors are also nearly identical, supporting the numerical validity of our Hessian-based uncertainty estimates.\n\n\n\nThe Poisson regression results provide insight into the firm-level factors associated with higher rates of patent awards.\n\nBlueprinty Customers: The coefficient for iscustomer is approximately 0.208, which is statistically significant. Interpreting this in the context of a Poisson model, we exponentiate the coefficient:\n[ e^{0.208} ] This implies that, holding other factors constant, firms using Blueprinty’s software are expected to receive 23% more patents on average than comparable non-customers.\nFirm Age: The negative coefficients on both age_std and age_sq_std suggest a concave relationship between firm age and patent production. Younger firms tend to have increasing returns to experience initially, but beyond a certain point, additional age is associated with diminishing patent activity.\nRegion: The regional dummy variables (relative to the Midwest) show small and statistically insignificant effects. This suggests that after controlling for age and customer status, regional location is not a strong predictor of patenting success in this dataset.\n\nOverall, the model supports Blueprinty’s marketing claim: even after controlling for firm characteristics, their customers tend to achieve higher rates of patent success.\n\n\n\nTo interpret the practical impact of Blueprinty’s software, we simulated two counterfactual scenarios:\n\nX_0: All firms are treated as non-customers (iscustomer = 0)\nX_1: All firms are treated as customers (iscustomer = 1)\n\nUsing the estimated Poisson regression coefficients, we predicted the number of patents for each firm in both scenarios and computed the average difference:\n[ = i ( {i, } - _{i, } ) ]\nThis means that, on average, using Blueprinty’s software is associated with nearly 0.8 more patents per firm over a 5-year period, controlling for firm age and region. This provides strong evidence in support of the software’s effectiveness.\n\nX_0 = X.copy()\nX_1 = X.copy()\n\n\niscustomer_col = [col for col in X.columns if \"iscustomer\" in col][0]\n\n\nX_0[iscustomer_col] = 0\nX_1[iscustomer_col] = 1\n\n\nX0_matrix = X_0.values\nX1_matrix = X_1.values\n\n\ny_pred_0 = np.exp(X0_matrix @ beta_hat)\ny_pred_1 = np.exp(X1_matrix @ beta_hat)\n\n\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\naverage_diff\n\n0.7958740085534443\n\n\n\n\n\nIn this case study, we investigated the relationship between the use of Blueprinty’s software and the number of patents awarded to engineering firms. Using a Poisson regression framework estimated via maximum likelihood, we found strong evidence that Blueprinty customers tend to receive more patents than non-customers—even after controlling for firm age and regional location.\nThe coefficient on the customer indicator was statistically significant and implied a 23% increase in expected patent counts. A counterfactual prediction exercise further suggested that, on average, using Blueprinty’s software is associated with nearly 0.8 additional patents per firm over a 5-year period.\nOverall, the results support the marketing team’s claim that Blueprinty’s product may enhance patent success. However, further research using experimental or longitudinal data would be helpful to more definitively establish causality."
  },
  {
    "objectID": "project/hw2/index.html#blueprinty-case-study",
    "href": "project/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n#| echo: true\n#| results: 'hide'\ndf.info(), df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n(None,\n    patents     region   age  iscustomer\n 0        0    Midwest  32.5           0\n 1        3  Southwest  37.5           0\n 2        4  Northwest  27.0           1\n 3        3  Northeast  24.5           0\n 4        3  Southwest  37.0           0)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nmean_patents = df.groupby(\"iscustomer\")[\"patents\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"stack\", palette=\"Set2\", bins=20)\nplt.xlabel(\"Number of Patents (Last 5 Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Distribution of Patents by Blueprinty Customer Status\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\nprint(\"Mean number of patents by customer status:\")\nprint(mean_patents.round(2))\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\niscustomer\nNon-Customers    3.47\nCustomers        4.13\nName: patents, dtype: float64\n\n\nThe chart above shows the distribution of the number of patents granted over the past five years, categorized by whether a company is a Blueprinty customer. We also calculated the average number of patents for each group:\n\nNon-customers: 3.47 patents\n\nCustomers: 4.13 patents\n\nBased on both the histogram and the means, companies that use Blueprinty’s software tend to hold more patents overall. This provides some support for the marketing team’s claim that Blueprinty customers are more successful in obtaining patents.\nHowever, this observation alone does not prove that the software causes higher success rates—because there may be systematic differences between customers and non-customers (such as company age or geographic location). Therefore, we need to further analyze these variables in the next steps.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo explore this possibility, we compared the regional distribution and firm age between the two groups.\nAs shown in the bar chart below, Blueprinty customers are not evenly distributed across regions. In particular, a notably higher proportion of customers come from the Northeast region, while other regions are more heavily populated by non-customers. This suggests that regional factors—such as industry concentration or market penetration—may influence software adoption.\nSimilarly, in terms of firm age, customers appear to be slightly older on average than non-customers (26.9 vs. 26.1 years). While the difference is modest, it indicates that older firms may be more likely to use Blueprinty’s software, potentially due to larger scale or greater administrative resources.\nThese findings support the idea that regional and demographic factors may confound any observed relationship between software use and patent success, and should be accounted for in further modeling.\n\nimport matplotlib.ticker as mtick\n\n\nplt.figure(figsize=(10, 5))\nregion_crosstab = pd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize=\"index\")\nregion_crosstab.plot(kind=\"bar\", stacked=True, color=[\"#d95f02\", \"#1b9e77\"])\nplt.title(\"Customer Distribution by Region\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Region\")\nplt.legend([\"Non-Customers\", \"Customers\"], title=\"Customer Status\")\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", multiple=\"stack\", bins=20, palette=\"Set2\")\nplt.title(\"Distribution of Firm Ages by Customer Status\")\nplt.xlabel(\"Firm Age (Years Since Incorporation)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# Mean ages\ndf.groupby(\"iscustomer\")[\"age\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n&lt;Figure size 960x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers    26.101570\nCustomers        26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe assume that the number of patents ( Y ) for each firm follows a Poisson distribution with rate parameter ( ):\n\\[\nY \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function (i.e., the likelihood for a single observation) is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nAssuming that observations are independent across firms, the likelihood for the full sample ( Y_1, Y_2, , Y_n ) is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nThis log-likelihood function is the basis for estimating the maximum likelihood value of ( ).\nThis log-likelihood function depends on both λ and the observed values Y.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, Y):\n\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nWe evaluated the Poisson log-likelihood over a grid of candidate λ values. As shown in the plot below, the log-likelihood is maximized at approximately λ = 3.68, which equals the sample mean.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nY = df[\"patents\"].values\n\n# Create a range of lambda values\nlambda_values = np.linspace(1, 7, 200)\nlog_likelihoods = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, log_likelihoods, color=\"blue\", label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color=\"red\", linestyle=\"--\", label=f\"Sample Mean = {np.mean(Y):.2f}\")\nplt.title(\"Poisson Log-Likelihood Curve\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe now take the analytical approach to derive the Maximum Likelihood Estimator (MLE) for ( ) under the Poisson model.\nRecall the log-likelihood function for independent observations ( Y_1, Y_2, , Y_n () ):\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTo find the MLE, we take the derivative of the log-likelihood with respect to ( ) and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = -n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThis result confirms that the MLE of ( ) under the Poisson distribution is simply the sample mean ( {Y} ), which is intuitive since ( [Y] = ) in the Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood (because we minimize)\ndef neg_log_likelihood(lmbda, Y):\n    return -poisson_log_likelihood(lmbda[0], Y)\n\n# Initial guess for lambda\ninitial_lambda = [2.0]\n\n# Run optimization\nresult = minimize(neg_log_likelihood, x0=initial_lambda, args=(Y,), bounds=[(1e-5, None)])\n\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846666035175017\n\n\nWe now use numerical optimization to find the value of λ that maximizes the Poisson log-likelihood. Since scipy.optimize.minimize() minimizes functions by default, we minimize the negative log-likelihood. The optimization result confirms our earlier finding: the MLE of λ is approximately equal to the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe define the following log-likelihood function for Poisson regression:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n\n    Xb = X @ beta  # linear predictor\n    lambdas = np.exp(Xb)  # inverse link\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport patsy\n\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n\n# Standardize age and age^2 (z-score, then round to 1 decimal place)\ndf[\"age_std\"] = ((df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()).round(1)\ndf[\"age_sq_std\"] = (df[\"age_std\"] ** 2).round(1)\n\n# Design matrix: intercept, standardized age, age^2, region dummies, iscustomer\nX = patsy.dmatrix(\"1 + age_std + age_sq_std + C(region, Treatment(reference='Midwest')) + iscustomer\", df, return_type=\"dataframe\")\nY = df[\"patents\"].values\nX_matrix = X.values\n\n# Define Poisson regression log-likelihood\ndef poisson_regression_loglikelihood(beta, Y, X):\n    Xb = X @ beta\n    lambdas = np.exp(Xb)\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n# Negative log-likelihood for optimization\ndef neg_loglik(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\n# Initial beta guess\ninitial_beta = np.zeros(X_matrix.shape[1])\n\n# Optimize with bounds and BFGS\nresult = minimize(neg_loglik, initial_beta, args=(Y, X_matrix), method=\"BFGS\", options={\"disp\": True})\n\n\nbeta_hat = result.x\nhessian = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian))\n\n\nsummary_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.design_info.column_names)\n\nOptimization terminated successfully.\n         Current function value: 3257.709028\n         Iterations: 18\n         Function evaluations: 243\n         Gradient evaluations: 27\n\n\nThe table below reports the estimated coefficients and standard errors from the Poisson regression model.\n\nThe coefficient on iscustomer is 0.2084 (SE = 0.0310), suggesting that Blueprinty customers are associated with a significantly higher rate of patent approvals, even after controlling for age and region.\nThe negative coefficients on both age_std and age_sq_std suggest a concave (inverted-U) relationship between firm age and patenting activity.\nRegional effects appear small and statistically insignificant relative to the Midwest baseline.\n\n\nsummary_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n1.3422\n0.0397\n\n\nC(region, Treatment(reference='Midwest'))[T.Northeast]\n0.0302\n0.0472\n\n\nC(region, Treatment(reference='Midwest'))[T.Northwest]\n-0.0179\n0.0575\n\n\nC(region, Treatment(reference='Midwest'))[T.South]\n0.0564\n0.0561\n\n\nC(region, Treatment(reference='Midwest'))[T.Southwest]\n0.0514\n0.0501\n\n\nage_std\n-0.0596\n0.0143\n\n\nage_sq_std\n-0.1549\n0.0142\n\n\niscustomer\n0.2084\n0.0310\n\n\n\n\n\n\n\nTo verify our results, we re-estimate the model using Python’s built-in GLM() function from the statsmodels package with a Poisson family.\n\nimport statsmodels.api as sm\nglm_model = sm.GLM(Y, X_matrix, family=sm.families.Poisson())\nglm_results = glm_model.fit()\nglm_summary_df = pd.DataFrame({\n    \"Coefficient (GLM)\": glm_results.params,\n    \"Std. Error (GLM)\": glm_results.bse\n}, index=X.design_info.column_names)\n\n# Summary table\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3257.7\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n2142.5\n\n\nTime:\n20:25:08\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1364\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.3422\n0.038\n35.054\n0.000\n1.267\n1.417\n\n\nx1\n0.0302\n0.044\n0.691\n0.489\n-0.055\n0.116\n\n\nx2\n-0.0179\n0.054\n-0.333\n0.739\n-0.123\n0.087\n\n\nx3\n0.0564\n0.053\n1.070\n0.285\n-0.047\n0.160\n\n\nx4\n0.0514\n0.047\n1.089\n0.276\n-0.041\n0.144\n\n\nx5\n-0.0596\n0.015\n-3.967\n0.000\n-0.089\n-0.030\n\n\nx6\n-0.1549\n0.013\n-11.515\n0.000\n-0.181\n-0.129\n\n\nx7\n0.2084\n0.031\n6.743\n0.000\n0.148\n0.269\n\n\n\n\n\nThe coefficient estimates from GLM() match those obtained via our custom maximum likelihood estimation, confirming the correctness of our implementation. Standard errors are also nearly identical, supporting the numerical validity of our Hessian-based uncertainty estimates.\n\n\n\nThe Poisson regression results provide insight into the firm-level factors associated with higher rates of patent awards.\n\nBlueprinty Customers: The coefficient for iscustomer is approximately 0.208, which is statistically significant. Interpreting this in the context of a Poisson model, we exponentiate the coefficient:\n[ e^{0.208} ] This implies that, holding other factors constant, firms using Blueprinty’s software are expected to receive 23% more patents on average than comparable non-customers.\nFirm Age: The negative coefficients on both age_std and age_sq_std suggest a concave relationship between firm age and patent production. Younger firms tend to have increasing returns to experience initially, but beyond a certain point, additional age is associated with diminishing patent activity.\nRegion: The regional dummy variables (relative to the Midwest) show small and statistically insignificant effects. This suggests that after controlling for age and customer status, regional location is not a strong predictor of patenting success in this dataset.\n\nOverall, the model supports Blueprinty’s marketing claim: even after controlling for firm characteristics, their customers tend to achieve higher rates of patent success.\n\n\n\nTo interpret the practical impact of Blueprinty’s software, we simulated two counterfactual scenarios:\n\nX_0: All firms are treated as non-customers (iscustomer = 0)\nX_1: All firms are treated as customers (iscustomer = 1)\n\nUsing the estimated Poisson regression coefficients, we predicted the number of patents for each firm in both scenarios and computed the average difference:\n[ = i ( {i, } - _{i, } ) ]\nThis means that, on average, using Blueprinty’s software is associated with nearly 0.8 more patents per firm over a 5-year period, controlling for firm age and region. This provides strong evidence in support of the software’s effectiveness.\n\nX_0 = X.copy()\nX_1 = X.copy()\n\n\niscustomer_col = [col for col in X.columns if \"iscustomer\" in col][0]\n\n\nX_0[iscustomer_col] = 0\nX_1[iscustomer_col] = 1\n\n\nX0_matrix = X_0.values\nX1_matrix = X_1.values\n\n\ny_pred_0 = np.exp(X0_matrix @ beta_hat)\ny_pred_1 = np.exp(X1_matrix @ beta_hat)\n\n\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\naverage_diff\n\n0.7958740085534443\n\n\n\n\n\nIn this case study, we investigated the relationship between the use of Blueprinty’s software and the number of patents awarded to engineering firms. Using a Poisson regression framework estimated via maximum likelihood, we found strong evidence that Blueprinty customers tend to receive more patents than non-customers—even after controlling for firm age and regional location.\nThe coefficient on the customer indicator was statistically significant and implied a 23% increase in expected patent counts. A counterfactual prediction exercise further suggested that, on average, using Blueprinty’s software is associated with nearly 0.8 additional patents per firm over a 5-year period.\nOverall, the results support the marketing team’s claim that Blueprinty’s product may enhance patent success. However, further research using experimental or longitudinal data would be helpful to more definitively establish causality."
  },
  {
    "objectID": "project/hw2/index.html#airbnb-case-study",
    "href": "project/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nAlthough true booking data is not available, we use the number of reviews as a proxy for the number of bookings, consistent with prior practice.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_airbnb = pd.read_csv(\"./airbnb.csv\")\n\n# Keep only variables of interest\nvars_of_interest = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf_airbnb_clean = df_airbnb[vars_of_interest].dropna()\n\nplt.figure(figsize=(8, 5))\nsns.histplot(df_airbnb_clean[\"number_of_reviews\"], bins=50)\nplt.xlim(0, 100)\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Number of Listings\")\nplt.title(\"Distribution of Airbnb Reviews (Capped at 100)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model\nTo model the number of reviews (as a proxy for bookings), we use a Poisson regression framework. The outcome is the count of reviews, and the predictors include listing characteristics, review scores, and room type.\n\nimport numpy as np\nimport patsy\nimport statsmodels.api as sm\n\n# Transform and encode\ndf_airbnb_clean[\"log_price\"] = np.log(df_airbnb_clean[\"price\"])\ndf_airbnb_clean[\"ibook\"] = (df_airbnb_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n# Design matrix with one-hot encoding for room type\nX_airbnb = patsy.dmatrix(\"1 + log_price + bedrooms + bathrooms + \"\n                         \"review_scores_cleanliness + review_scores_location + \"\n                         \"review_scores_value + ibook + C(room_type)\",\n                         df_airbnb_clean, return_type=\"dataframe\")\nY_airbnb = df_airbnb_clean[\"number_of_reviews\"].values\n\n# Poisson model\npoisson_model = sm.GLM(Y_airbnb, X_airbnb, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\npoisson_results.summary2().tables[1]  # Clean coefficient table\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3.075887\n0.019149\n160.627772\n0.000000e+00\n3.038356\n3.113419\n\n\nC(room_type)[T.Private room]\n0.085839\n0.003377\n25.421373\n1.463838e-142\n0.079221\n0.092457\n\n\nC(room_type)[T.Shared room]\n-0.105034\n0.009105\n-11.535661\n8.721427e-31\n-0.122880\n-0.087188\n\n\nlog_price\n0.134508\n0.002879\n46.712984\n0.000000e+00\n0.128864\n0.140152\n\n\nbedrooms\n0.046661\n0.002038\n22.900255\n4.619038e-116\n0.042667\n0.050655\n\n\nbathrooms\n-0.152019\n0.003742\n-40.620057\n0.000000e+00\n-0.159354\n-0.144684\n\n\nreview_scores_cleanliness\n0.108745\n0.001496\n72.677404\n0.000000e+00\n0.105813\n0.111678\n\n\nreview_scores_location\n-0.097710\n0.001649\n-59.239789\n0.000000e+00\n-0.100943\n-0.094477\n\n\nreview_scores_value\n-0.079629\n0.001820\n-43.757618\n0.000000e+00\n-0.083196\n-0.076063\n\n\nibook\n0.340836\n0.002891\n117.879132\n0.000000e+00\n0.335169\n0.346503\n\n\n\n\n\n\n\n\n\nInterpretation of Results\nThe regression results offer insight into what drives more bookings (as proxied by reviews):\nPrice: The coefficient on log_price is negative, indicating that higher prices are associated with fewer bookings.\nRoom Type: Shared and private rooms tend to receive fewer bookings compared to entire apartments, all else equal.\nCleanliness, Location, Value: Higher review scores on these dimensions are significantly associated with more bookings, especially “value” and “location.”\nInstant Bookable: Listings that are instantly bookable see a meaningful increase in expected bookings, suggesting customers prefer lower friction when booking.\n\n\nMarginal Effect of Instant Bookable\nTo estimate the practical impact of making a listing instantly bookable, we conduct a counterfactual simulation:\nX_0: All listings are set to not instantly bookable (ibook = 0)\nX_1: All listings are set to instantly bookable (ibook = 1)\nWe use the fitted Poisson regression model to predict the number of reviews in both cases and compute the average difference.\n\n# Create copies of design matrix\nX0_airbnb = X_airbnb.copy()\nX1_airbnb = X_airbnb.copy()\n\n# Identify the column corresponding to 'ibook'\nibook_col = [col for col in X_airbnb.columns if \"ibook\" in col][0]\n\n# Set to 0 and 1 for the two counterfactuals\nX0_airbnb[ibook_col] = 0\nX1_airbnb[ibook_col] = 1\n\n# Predict expected reviews under each scenario\npred_reviews_0 = np.exp(X0_airbnb @ poisson_results.params)\npred_reviews_1 = np.exp(X1_airbnb @ poisson_results.params)\n\n# Calculate difference\naverage_increase = np.mean(pred_reviews_1 - pred_reviews_0)\naverage_increase\n\n7.964686182144014\n\n\nThis simulation suggests that, on average, enabling Instant Bookable leads to approximately {average_increase:.2f} more reviews per listing, holding all other characteristics constant. This reinforces the importance of reducing booking friction to increase customer engagement on the platform.\n\n\nConclusion\nThis analysis explored what drives variation in the number of Airbnb reviews—used as a proxy for bookings—across listings in New York City. Using Poisson regression, we find that several factors significantly predict higher booking rates:\nPrice is negatively associated with reviews, suggesting that more expensive listings tend to receive fewer bookings.\nReview scores on cleanliness, location, and value are strong predictors of booking volume, especially value and location.\nInstant Bookable status meaningfully increases expected bookings. A counterfactual simulation showed that enabling instant booking is associated with an average increase of roughly X reviews per listing.\nThese findings suggest that Airbnb hosts can boost their success by improving perceived value, maintaining high review quality, and enabling instant booking where possible."
  }
]