[
  {
    "objectID": "project/hw1/index.html",
    "href": "project/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe core hypothesis explored in the experiment is whether offering matching donations increases the likelihood and size of charitable contributions. Matching donations are often used by nonprofits under the belief that people are more likely to give when they know their donation will be “matched” by another funder.\nTo test this idea, Karlan and List conducted a large-scale natural field experiment involving 50,083 individuals who had previously donated to a politically conservative nonprofit organization. These individuals were randomly assigned to receive fundraising letters with different conditions: - Control group: received a standard donation request. - Treatment group: received one of three types of matching grants — a 1:1, 2:1, or 3:1 match. - Each letter also varied the match threshold (i.e., how much the funder was willing to match in total) and the suggested donation amount (either equal to or a multiplier of the donor’s previous contribution).\nThe strength of this design lies in its scale, natural setting (real donors, real money), and random assignment, which allows for clean causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "project/hw1/index.html#introduction",
    "href": "project/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe core hypothesis explored in the experiment is whether offering matching donations increases the likelihood and size of charitable contributions. Matching donations are often used by nonprofits under the belief that people are more likely to give when they know their donation will be “matched” by another funder.\nTo test this idea, Karlan and List conducted a large-scale natural field experiment involving 50,083 individuals who had previously donated to a politically conservative nonprofit organization. These individuals were randomly assigned to receive fundraising letters with different conditions: - Control group: received a standard donation request. - Treatment group: received one of three types of matching grants — a 1:1, 2:1, or 3:1 match. - Each letter also varied the match threshold (i.e., how much the funder was willing to match in total) and the suggested donation amount (either equal to or a multiplier of the donor’s previous contribution).\nThe strength of this design lies in its scale, natural setting (real donors, real money), and random assignment, which allows for clean causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "project/hw1/index.html#data",
    "href": "project/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe analyze the dataset provided by the authors, which contains 50,083 observations — each representing a recipient of a fundraising letter. The dataset includes variables describing:\n\nExperimental assignment: whether the individual received a control or treatment letter, and what kind of match ratio they received\nPast donation behavior: frequency, amount, time since last donation, etc.\nDemographics and political geography\nThe outcome variables: whether the individual donated and how much they gave\n\nBelow is the code used to load the dataset into Python and show a snapshot of its structure:\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Display shape and variable names\ndf.shape, df.columns.tolist()\n\n((50083, 51),\n ['treatment',\n  'control',\n  'ratio',\n  'ratio2',\n  'ratio3',\n  'size',\n  'size25',\n  'size50',\n  'size100',\n  'sizeno',\n  'ask',\n  'askd1',\n  'askd2',\n  'askd3',\n  'ask1',\n  'ask2',\n  'ask3',\n  'amount',\n  'gave',\n  'amountchange',\n  'hpa',\n  'ltmedmra',\n  'freq',\n  'years',\n  'year5',\n  'mrm2',\n  'dormant',\n  'female',\n  'couple',\n  'state50one',\n  'nonlit',\n  'cases',\n  'statecnt',\n  'stateresponse',\n  'stateresponset',\n  'stateresponsec',\n  'stateresponsetminc',\n  'perbush',\n  'close25',\n  'red0',\n  'blue0',\n  'redcty',\n  'bluecty',\n  'pwhite',\n  'pblack',\n  'page18_39',\n  'ave_hh_sz',\n  'median_hhincome',\n  'powner',\n  'psch_atlstba',\n  'pop_propurban'])\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWe begin by checking whether the treatment and control groups are balanced in terms of observable characteristics. This is a common first step in analyzing randomized experiments: if randomization was implemented correctly, both groups should be similar on all baseline variables.\nHere, we focus on one such variable: the number of months since a donor’s last contribution (mrm2).\nThe average for the control group is 12.99 months, while for the treatment group it is 13.01 months. A t-test comparing these means yields t = 0.12, p = 0.905, indicating no statistically significant difference.\nWe also estimate a linear regression of mrm2 on the treatment indicator. The coefficient is effectively zero and not statistically significant, confirming the same conclusion.\nThese results suggest that the randomization was successful: the two groups appear well-balanced with respect to prior giving behavior. This gives us confidence that subsequent differences in donation outcomes can be attributed to the experimental treatments.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n#  treatment + control + mrm2\ndf_subset = df[[\"treatment\", \"control\", \"mrm2\"]].dropna()\n\n# \nmeans = df_subset.groupby(\"treatment\")[\"mrm2\"].mean()\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df_subset[df_subset[\"treatment\"] == 1][\"mrm2\"],\n    df_subset[df_subset[\"treatment\"] == 0][\"mrm2\"],\n    equal_var=False  # Welch's t-test\n)\n\n\nX = sm.add_constant(df_subset[\"treatment\"])\nmodel = sm.OLS(df_subset[\"mrm2\"], X).fit()\n\n\nprint(\"=== mean value ===\")\nprint(means)\nprint(\"\\n=== t-test ===\")\nprint(f\"t = {t_stat:.3f}, p = {p_value:.3f}\")\nprint(\"\\n=== OLS Regression result ===\")\nprint(model.summary())\n\n=== mean value ===\ntreatment\n0    12.998142\n1    13.011828\nName: mrm2, dtype: float64\n\n=== t-test ===\nt = 0.120, p = 0.905\n\n=== OLS Regression result ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Sat, 07 Jun 2025   Prob (F-statistic):              0.905\nTime:                        15:43:52   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\nWhy check for balance?\n\n\n\nBalance tests help confirm that the randomization worked as expected. If the treatment and control groups are similar on baseline characteristics, we can be more confident that later differences in donation outcomes are due to the treatment and not underlying differences between groups."
  },
  {
    "objectID": "project/hw1/index.html#experimental-results",
    "href": "project/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe begin by comparing donation rates between the treatment and control groups. In the graph below, we see the proportion of individuals who gave any donation, split by whether they received a matching offer or not.\n\nimport statsmodels.formula.api as smf\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df[df[\"treatment\"] == 1][\"gave\"],\n    df[df[\"treatment\"] == 0][\"gave\"],\n    equal_var=False\n)\n\n# linear regression\nols_model = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Probit \nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\n\n\nfig, ax = plt.subplots(figsize=(6,4))\ngave_by_group.plot(kind='bar', color=[\"pink\", \"blue\"], ax=ax)\nax.set_ylabel(\"Proportion Donated\")\nax.set_title(\"Donation Rate by Treatment Group\")\nax.set_xticklabels([\"Control\", \"Treatment\"], rotation=0)\nax.set_ylim(0, 0.05)\nplt.tight_layout()\n\n\nprint(\"t =\", t_stat, \"p =\", p_value)\n\nprint(ols_model.summary2().tables[1])\n\nprint(probit_model.summary2().tables[1])\n\nt = 3.2094621908279835 p = 0.001330982345091417\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  0.017858  0.001101  16.224643  4.779032e-59  0.015701  0.020016\ntreatment  0.004180  0.001348   3.101361  1.927403e-03  0.001538  0.006822\n              Coef.  Std.Err.         z     P&gt;|z|    [0.025    0.975]\nIntercept -2.100141  0.023316 -90.07277  0.000000 -2.145840 -2.054443\ntreatment  0.086785  0.027879   3.11293  0.001852  0.032143  0.141426\n\n\n\n\n\n\n\n\n\nThe donation rate for the control group is approximately 1.8%, while for the treatment group it is 2.2%. This difference is statistically significant:\nA t-test yields t = 3.21, p = 0.0013, indicating that the observed difference is unlikely to be due to random chance.\nA linear probability model (OLS) confirms this result with a positive and significant coefficient on treatment:\n\nols_model.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.017858\n0.001101\n16.224643\n4.779032e-59\n0.015701\n0.020016\n\n\ntreatment\n0.004180\n0.001348\n3.101361\n1.927403e-03\n0.001538\n0.006822\n\n\n\n\n\n\n\nTo further validate the result, we estimate a probit model of donation on treatment status. The probit coefficient is 0.087 (p = 0.0019), again confirming a statistically significant effect of matching offers.\n\nprobit_model.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-2.100141\n0.023316\n-90.07277\n0.000000\n-2.145840\n-2.054443\n\n\ntreatment\n0.086785\n0.027879\n3.11293\n0.001852\n0.032143\n0.141426\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nThese results suggest that offering a matching donation significantly increases the likelihood that a person will donate. From a behavioral standpoint, this provides evidence that people are more motivated to give when they feel their contribution will be amplified.\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nT-test Comparisons To examine whether the size of the match ratio affects donation behavior, I restrict the sample to the treatment group and compute the donation rate for each ratio subgroup:\n1:1 match ratio → 2.07%\n2:1 match ratio → 2.02%\n3:1 match ratio → 2.27%\nI then perform t-tests to compare the donation rates between different match sizes:\n1:1 vs 2:1: p = 0.3345\n2:1 vs 3:1: p = 0.9600\nThese p-values indicate that the differences in response rates across match ratios are not statistically significant. Therefore, larger match ratios do not appear to generate significantly higher giving rates, consistent with what Karlan & List describe on page 8: “the figures suggest no consistent pattern.”\nRegression Analysis To validate this further, I estimate a linear regression model with dummy variables indicating the different match ratios:\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf[\"ratio1\"] = (df[\"ratio\"] == 1).astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == 2).astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == 3).astype(int)\n\n\ndf_matched = df[df[\"treatment\"] == 1]\n\n\nX = df_matched[[\"ratio1\", \"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df_matched[\"gave\"]\n\nmodel = sm.OLS(y, X).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n-1.597075e+09\n3.428594e+11\n-0.004658\n0.996283\n-6.736135e+11\n6.704193e+11\n\n\nratio1\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio2\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio3\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\n\n\n\n\n\nThe regression on gave using these dummies among the treatment group yields:\nAll coefficients are not statistically significant (p &gt; 0.99)\nCoefficients are extremely small, confirming the weak impact of match ratio levels on response behavior\nDirect Comparison of Response Rates Finally, I directly compute the differences in average donation rates:\n2:1 – 1:1 difference = 0.0019\n3:1 – 2:1 difference = 0.0001\nAgain, the numerical differences are very small, further supporting the conclusion that increasing the match ratio from 1:1 to 3:1 does not lead to materially greater giving.\n\n\n\n\n\n\nConclusion\n\n\n\nTaken together, these results replicate the original findings: while matched donations do increase overall giving compared to no match, increasing the generosity of the match offer (from 1:1 to 3:1) does not significantly boost response rates.\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nFull Sample We begin by regressing the donation amount on the treatment status across the full sample. The estimated coefficient on the treatment dummy is 0.154, but the p-value is approximately 0.063, which suggests the result is not statistically significant at the 5% level. This implies that when considering all individuals, offering a matching grant may increase donation amounts, but the evidence is not conclusive.\nConditional on Donating Next, we restrict the sample to only those who made a donation. This allows us to analyze how much respondents donate conditional on giving something. The treatment coefficient becomes -1.669, with a p-value of 0.561, indicating no statistically significant difference in the donation size between treatment and control groups among those who chose to give.\nThis suggests that while the offer of a match may increase the likelihood of donating (as shown earlier), it does not significantly impact the amount donated by those who do choose to donate.\nDistributional Plots We also visualize the distribution of donation amounts for treatment and control groups using histograms:\n\nimport matplotlib.pyplot as plt\n\n\ncontrol_amounts = df[(df[\"treatment\"] == 0) & (df[\"gave\"] == 1)][\"amount\"]\ntreatment_amounts = df[(df[\"treatment\"] == 1) & (df[\"gave\"] == 1)][\"amount\"]\n\n# Plot for control group\nfig, ax = plt.subplots()\nax.hist(control_amounts, bins=30, alpha=0.7, label=\"Control\", color=\"skyblue\")\nax.axvline(control_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Control Mean\")\nax.set_title(\"Donation Amounts: Control Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig\n\n# Plot for treatment group\nfig, ax = plt.subplots()\nax.hist(treatment_amounts, bins=30, alpha=0.7, label=\"Treatment\", color=\"lightgreen\")\nax.axvline(treatment_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Treatment Mean\")\nax.set_title(\"Donation Amounts: Treatment Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig"
  },
  {
    "objectID": "project/hw1/index.html#simulation-experiment",
    "href": "project/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThis chart illustrates the Law of Large Numbers using a simulated experiment. I generated 10,000 simulated donation outcomes for the control group (Bernoulli p = 0.018) and 10,000 for the treatment group (Bernoulli p = 0.022), and then computed the cumulative average of their differences.\nAs shown, the cumulative average is quite noisy at the beginning due to the small sample size — early differences fluctuate dramatically. However, as the number of observations increases, the cumulative average stabilizes around 0.004 (the true treatment effect), demonstrating convergence.\nThis supports the intuition behind large sample inference: as the sample size grows, random variation diminishes, and estimates converge to their true values.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\np_control = 0.018\np_treatment = 0.022\nn = 10_000\n\n\nnp.random.seed(42)\ncontrol = np.random.binomial(n=1, p=p_control, size=n)\ntreatment = np.random.binomial(n=1, p=p_treatment, size=n)\n\n\ndiff = treatment - control\ncumulative_avg = np.cumsum(diff) / np.arange(1, n + 1)\n\n# Plot \nfig, ax = plt.subplots()\nax.plot(cumulative_avg, label=\"Cumulative Average\")\nax.axhline(y=0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nax.set_title(\"Law of Large Numbers: Cumulative Average of Differences\")\nax.set_xlabel(\"Sample Size\")\nax.set_ylabel(\"Cumulative Average\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(123)\n\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\n\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 4))\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(1000):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treatment, n)\n        diff = treatment.mean() - control.mean()\n        differences.append(diff)\n\n    ax = axs[i // 2, i % 2]\n    ax.hist(differences, bins=30, color=\"grey\", edgecolor=\"black\")\n    ax.set_title(f\"Sample Size: {n}\")\n    ax.set_xlabel(\"Average Difference\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown above, these histograms display the distribution of average differences across 1000 simulations for various sample sizes. When the sample size is small (e.g., 50), the distribution is highly variable and not symmetric. As the sample size increases, the distributions become increasingly symmetric and bell-shaped, illustrating the Central Limit Theorem: the sampling distribution of the sample mean approaches a normal distribution as the sample size increases."
  },
  {
    "objectID": "project/hw2/index.html",
    "href": "project/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n#| echo: true\n#| results: 'hide'\ndf.info(), df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n(None,\n    patents     region   age  iscustomer\n 0        0    Midwest  32.5           0\n 1        3  Southwest  37.5           0\n 2        4  Northwest  27.0           1\n 3        3  Northeast  24.5           0\n 4        3  Southwest  37.0           0)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nmean_patents = df.groupby(\"iscustomer\")[\"patents\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"stack\", palette=\"Set2\", bins=20, alpha=0.8)\nplt.xlabel(\"Number of Patents (Last 5 Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Distribution of Patents by Blueprinty Customer Status\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\nprint(\"Mean number of patents by customer status:\")\nprint(mean_patents.round(2))\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\niscustomer\nNon-Customers    3.47\nCustomers        4.13\nName: patents, dtype: float64\n\n\nThe chart above shows the distribution of the number of patents granted over the past five years, categorized by whether a company is a Blueprinty customer. We also calculated the average number of patents for each group:\n\nNon-customers: 3.47 patents\n\nCustomers: 4.13 patents\n\nBased on both the histogram and the means, companies that use Blueprinty’s software tend to hold more patents overall. This provides some support for the marketing team’s claim that Blueprinty customers are more successful in obtaining patents.\nHowever, this observation alone does not prove that the software causes higher success rates—because there may be systematic differences between customers and non-customers (such as company age or geographic location). Therefore, we need to further analyze these variables in the next steps.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo explore this possibility, we compared the regional distribution and firm age between the two groups.\nAs shown in the bar chart below, Blueprinty customers are not evenly distributed across regions. In particular, a notably higher proportion of customers come from the Northeast region, while other regions are more heavily populated by non-customers. This suggests that regional factors—such as industry concentration or market penetration—may influence software adoption.\nSimilarly, in terms of firm age, customers appear to be slightly older on average than non-customers (26.9 vs. 26.1 years). While the difference is modest, it indicates that older firms may be more likely to use Blueprinty’s software, potentially due to larger scale or greater administrative resources.\nThese findings support the idea that regional and demographic factors may confound any observed relationship between software use and patent success, and should be accounted for in further modeling.\n\nimport matplotlib.ticker as mtick\n\n\nplt.figure(figsize=(10, 5))\nregion_crosstab = pd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize=\"index\")\nregion_crosstab.plot(kind=\"bar\", stacked=True, color=[\"#d95f02\", \"#1b9e77\"])\nplt.title(\"Customer Distribution by Region\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Region\")\nplt.legend([\"Non-Customers\", \"Customers\"], title=\"Customer Status\")\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", multiple=\"stack\", bins=20, palette=\"Set2\", alpha=0.8)\nplt.title(\"Distribution of Firm Ages by Customer Status\")\nplt.xlabel(\"Firm Age (Years Since Incorporation)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# Mean ages\ndf.groupby(\"iscustomer\")[\"age\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n&lt;Figure size 960x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers    26.101570\nCustomers        26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe assume that the number of patents ( Y ) for each firm follows a Poisson distribution with rate parameter ( ):\n\\[\nY \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function (i.e., the likelihood for a single observation) is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nAssuming that observations are independent across firms, the likelihood for the full sample ( Y_1, Y_2, , Y_n ) is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nThis log-likelihood function is the basis for estimating the maximum likelihood value of ( ).\nThis log-likelihood function depends on both λ and the observed values Y.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, Y):\n\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nWe evaluated the Poisson log-likelihood over a grid of candidate λ values. As shown in the plot below, the log-likelihood is maximized at approximately λ = 3.68, which equals the sample mean.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nY = df[\"patents\"].values\n\n# Create a range of lambda values\nlambda_values = np.linspace(1, 7, 200)\nlog_likelihoods = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, log_likelihoods, color=\"blue\", label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color=\"red\", linestyle=\"--\", label=f\"Sample Mean = {np.mean(Y):.2f}\")\nplt.title(\"Poisson Log-Likelihood Curve\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe now take the analytical approach to derive the Maximum Likelihood Estimator (MLE) for ( ) under the Poisson model.\nRecall the log-likelihood function for independent observations ( Y_1, Y_2, , Y_n () ):\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTo find the MLE, we take the derivative of the log-likelihood with respect to ( ) and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = -n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThis result confirms that the MLE of ( ) under the Poisson distribution is simply the sample mean ( {Y} ), which is intuitive since ( [Y] = ) in the Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood (because we minimize)\ndef neg_log_likelihood(lmbda, Y):\n    return -poisson_log_likelihood(lmbda[0], Y)\n\n# Initial guess for lambda\ninitial_lambda = [2.0]\n\n# Run optimization\nresult = minimize(neg_log_likelihood, x0=initial_lambda, args=(Y,), bounds=[(1e-5, None)])\n\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846666035175017\n\n\nWe now use numerical optimization to find the value of λ that maximizes the Poisson log-likelihood. Since scipy.optimize.minimize() minimizes functions by default, we minimize the negative log-likelihood. The optimization result confirms our earlier finding: the MLE of λ is approximately equal to the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe define the following log-likelihood function for Poisson regression:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n\n    Xb = X @ beta  # linear predictor\n    lambdas = np.exp(Xb)  # inverse link\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport patsy\n\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n\n# Standardize age and age^2 (z-score, then round to 1 decimal place)\ndf[\"age_std\"] = ((df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()).round(1)\ndf[\"age_sq_std\"] = (df[\"age_std\"] ** 2).round(1)\n\n# Design matrix: intercept, standardized age, age^2, region dummies, iscustomer\nX = patsy.dmatrix(\"1 + age_std + age_sq_std + C(region, Treatment(reference='Midwest')) + iscustomer\", df, return_type=\"dataframe\")\nY = df[\"patents\"].values\nX_matrix = X.values\n\n# Define Poisson regression log-likelihood\ndef poisson_regression_loglikelihood(beta, Y, X):\n    Xb = X @ beta\n    lambdas = np.exp(Xb)\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n# Negative log-likelihood for optimization\ndef neg_loglik(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\n# Initial beta guess\ninitial_beta = np.zeros(X_matrix.shape[1])\n\n# Optimize with bounds and BFGS\nresult = minimize(neg_loglik, initial_beta, args=(Y, X_matrix), method=\"BFGS\", options={\"disp\": True})\n\n\nbeta_hat = result.x\nhessian = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian))\n\n\nsummary_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.design_info.column_names)\n\nOptimization terminated successfully.\n         Current function value: 3257.709028\n         Iterations: 18\n         Function evaluations: 243\n         Gradient evaluations: 27\n\n\nThe table below reports the estimated coefficients and standard errors from the Poisson regression model.\n\nThe coefficient on iscustomer is 0.2084 (SE = 0.0310), suggesting that Blueprinty customers are associated with a significantly higher rate of patent approvals, even after controlling for age and region.\nThe negative coefficients on both age_std and age_sq_std suggest a concave (inverted-U) relationship between firm age and patenting activity.\nRegional effects appear small and statistically insignificant relative to the Midwest baseline.\n\n\nsummary_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n1.3422\n0.0397\n\n\nC(region, Treatment(reference='Midwest'))[T.Northeast]\n0.0302\n0.0472\n\n\nC(region, Treatment(reference='Midwest'))[T.Northwest]\n-0.0179\n0.0575\n\n\nC(region, Treatment(reference='Midwest'))[T.South]\n0.0564\n0.0561\n\n\nC(region, Treatment(reference='Midwest'))[T.Southwest]\n0.0514\n0.0501\n\n\nage_std\n-0.0596\n0.0143\n\n\nage_sq_std\n-0.1549\n0.0142\n\n\niscustomer\n0.2084\n0.0310\n\n\n\n\n\n\n\nTo verify our results, we re-estimate the model using Python’s built-in GLM() function from the statsmodels package with a Poisson family.\n\nimport statsmodels.api as sm\nglm_model = sm.GLM(Y, X_matrix, family=sm.families.Poisson())\nglm_results = glm_model.fit()\nglm_summary_df = pd.DataFrame({\n    \"Coefficient (GLM)\": glm_results.params,\n    \"Std. Error (GLM)\": glm_results.bse\n}, index=X.design_info.column_names)\n\n# Summary table\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3257.7\n\n\nDate:\nSat, 07 Jun 2025\nDeviance:\n2142.5\n\n\nTime:\n15:43:48\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1364\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.3422\n0.038\n35.054\n0.000\n1.267\n1.417\n\n\nx1\n0.0302\n0.044\n0.691\n0.489\n-0.055\n0.116\n\n\nx2\n-0.0179\n0.054\n-0.333\n0.739\n-0.123\n0.087\n\n\nx3\n0.0564\n0.053\n1.070\n0.285\n-0.047\n0.160\n\n\nx4\n0.0514\n0.047\n1.089\n0.276\n-0.041\n0.144\n\n\nx5\n-0.0596\n0.015\n-3.967\n0.000\n-0.089\n-0.030\n\n\nx6\n-0.1549\n0.013\n-11.515\n0.000\n-0.181\n-0.129\n\n\nx7\n0.2084\n0.031\n6.743\n0.000\n0.148\n0.269\n\n\n\n\n\nThe coefficient estimates from GLM() match those obtained via our custom maximum likelihood estimation, confirming the correctness of our implementation. Standard errors are also nearly identical, supporting the numerical validity of our Hessian-based uncertainty estimates.\n\n\n\nThe Poisson regression results provide insight into the firm-level factors associated with higher rates of patent awards.\n\nBlueprinty Customers: The coefficient for iscustomer is approximately 0.208, which is statistically significant. Interpreting this in the context of a Poisson model, we exponentiate the coefficient:\n[ e^{0.208} ] This implies that, holding other factors constant, firms using Blueprinty’s software are expected to receive 23% more patents on average than comparable non-customers.\nFirm Age: The negative coefficients on both age_std and age_sq_std suggest a concave relationship between firm age and patent production. Younger firms tend to have increasing returns to experience initially, but beyond a certain point, additional age is associated with diminishing patent activity.\nRegion: The regional dummy variables (relative to the Midwest) show small and statistically insignificant effects. This suggests that after controlling for age and customer status, regional location is not a strong predictor of patenting success in this dataset.\n\nOverall, the model supports Blueprinty’s marketing claim: even after controlling for firm characteristics, their customers tend to achieve higher rates of patent success.\n\n\n\nTo interpret the practical impact of Blueprinty’s software, we simulated two counterfactual scenarios:\n\nX_0: All firms are treated as non-customers (iscustomer = 0)\nX_1: All firms are treated as customers (iscustomer = 1)\n\nUsing the estimated Poisson regression coefficients, we predicted the number of patents for each firm in both scenarios and computed the average difference:\n[ = i ( {i, } - _{i, } ) ]\nThis means that, on average, using Blueprinty’s software is associated with nearly 0.8 more patents per firm over a 5-year period, controlling for firm age and region. This provides strong evidence in support of the software’s effectiveness.\n\nX_0 = X.copy()\nX_1 = X.copy()\n\n\niscustomer_col = [col for col in X.columns if \"iscustomer\" in col][0]\n\n\nX_0[iscustomer_col] = 0\nX_1[iscustomer_col] = 1\n\n\nX0_matrix = X_0.values\nX1_matrix = X_1.values\n\n\ny_pred_0 = np.exp(X0_matrix @ beta_hat)\ny_pred_1 = np.exp(X1_matrix @ beta_hat)\n\n\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\naverage_diff\n\n0.7958740085534443\n\n\n\n\n\nIn this case study, we investigated the relationship between the use of Blueprinty’s software and the number of patents awarded to engineering firms. Using a Poisson regression framework estimated via maximum likelihood, we found strong evidence that Blueprinty customers tend to receive more patents than non-customers—even after controlling for firm age and regional location.\nThe coefficient on the customer indicator was statistically significant and implied a 23% increase in expected patent counts. A counterfactual prediction exercise further suggested that, on average, using Blueprinty’s software is associated with nearly 0.8 additional patents per firm over a 5-year period.\nOverall, the results support the marketing team’s claim that Blueprinty’s product may enhance patent success. However, further research using experimental or longitudinal data would be helpful to more definitively establish causality."
  },
  {
    "objectID": "project/hw2/index.html#blueprinty-case-study",
    "href": "project/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n#| echo: true\n#| results: 'hide'\ndf.info(), df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.0+ KB\n\n\n(None,\n    patents     region   age  iscustomer\n 0        0    Midwest  32.5           0\n 1        3  Southwest  37.5           0\n 2        4  Northwest  27.0           1\n 3        3  Northeast  24.5           0\n 4        3  Southwest  37.0           0)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nmean_patents = df.groupby(\"iscustomer\")[\"patents\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"stack\", palette=\"Set2\", bins=20, alpha=0.8)\nplt.xlabel(\"Number of Patents (Last 5 Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Distribution of Patents by Blueprinty Customer Status\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\nprint(\"Mean number of patents by customer status:\")\nprint(mean_patents.round(2))\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\niscustomer\nNon-Customers    3.47\nCustomers        4.13\nName: patents, dtype: float64\n\n\nThe chart above shows the distribution of the number of patents granted over the past five years, categorized by whether a company is a Blueprinty customer. We also calculated the average number of patents for each group:\n\nNon-customers: 3.47 patents\n\nCustomers: 4.13 patents\n\nBased on both the histogram and the means, companies that use Blueprinty’s software tend to hold more patents overall. This provides some support for the marketing team’s claim that Blueprinty customers are more successful in obtaining patents.\nHowever, this observation alone does not prove that the software causes higher success rates—because there may be systematic differences between customers and non-customers (such as company age or geographic location). Therefore, we need to further analyze these variables in the next steps.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo explore this possibility, we compared the regional distribution and firm age between the two groups.\nAs shown in the bar chart below, Blueprinty customers are not evenly distributed across regions. In particular, a notably higher proportion of customers come from the Northeast region, while other regions are more heavily populated by non-customers. This suggests that regional factors—such as industry concentration or market penetration—may influence software adoption.\nSimilarly, in terms of firm age, customers appear to be slightly older on average than non-customers (26.9 vs. 26.1 years). While the difference is modest, it indicates that older firms may be more likely to use Blueprinty’s software, potentially due to larger scale or greater administrative resources.\nThese findings support the idea that regional and demographic factors may confound any observed relationship between software use and patent success, and should be accounted for in further modeling.\n\nimport matplotlib.ticker as mtick\n\n\nplt.figure(figsize=(10, 5))\nregion_crosstab = pd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize=\"index\")\nregion_crosstab.plot(kind=\"bar\", stacked=True, color=[\"#d95f02\", \"#1b9e77\"])\nplt.title(\"Customer Distribution by Region\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Region\")\nplt.legend([\"Non-Customers\", \"Customers\"], title=\"Customer Status\")\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", multiple=\"stack\", bins=20, palette=\"Set2\", alpha=0.8)\nplt.title(\"Distribution of Firm Ages by Customer Status\")\nplt.xlabel(\"Firm Age (Years Since Incorporation)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# Mean ages\ndf.groupby(\"iscustomer\")[\"age\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n&lt;Figure size 960x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers    26.101570\nCustomers        26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe assume that the number of patents ( Y ) for each firm follows a Poisson distribution with rate parameter ( ):\n\\[\nY \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function (i.e., the likelihood for a single observation) is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nAssuming that observations are independent across firms, the likelihood for the full sample ( Y_1, Y_2, , Y_n ) is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nThis log-likelihood function is the basis for estimating the maximum likelihood value of ( ).\nThis log-likelihood function depends on both λ and the observed values Y.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, Y):\n\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nWe evaluated the Poisson log-likelihood over a grid of candidate λ values. As shown in the plot below, the log-likelihood is maximized at approximately λ = 3.68, which equals the sample mean.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nY = df[\"patents\"].values\n\n# Create a range of lambda values\nlambda_values = np.linspace(1, 7, 200)\nlog_likelihoods = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, log_likelihoods, color=\"blue\", label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color=\"red\", linestyle=\"--\", label=f\"Sample Mean = {np.mean(Y):.2f}\")\nplt.title(\"Poisson Log-Likelihood Curve\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe now take the analytical approach to derive the Maximum Likelihood Estimator (MLE) for ( ) under the Poisson model.\nRecall the log-likelihood function for independent observations ( Y_1, Y_2, , Y_n () ):\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTo find the MLE, we take the derivative of the log-likelihood with respect to ( ) and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = -n + \\frac{\\sum_{i=1}^n Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nThis result confirms that the MLE of ( ) under the Poisson distribution is simply the sample mean ( {Y} ), which is intuitive since ( [Y] = ) in the Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood (because we minimize)\ndef neg_log_likelihood(lmbda, Y):\n    return -poisson_log_likelihood(lmbda[0], Y)\n\n# Initial guess for lambda\ninitial_lambda = [2.0]\n\n# Run optimization\nresult = minimize(neg_log_likelihood, x0=initial_lambda, args=(Y,), bounds=[(1e-5, None)])\n\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846666035175017\n\n\nWe now use numerical optimization to find the value of λ that maximizes the Poisson log-likelihood. Since scipy.optimize.minimize() minimizes functions by default, we minimize the negative log-likelihood. The optimization result confirms our earlier finding: the MLE of λ is approximately equal to the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe define the following log-likelihood function for Poisson regression:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n\n    Xb = X @ beta  # linear predictor\n    lambdas = np.exp(Xb)  # inverse link\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport patsy\n\n\ndf = pd.read_csv(\"./blueprinty.csv\")\n\n# Standardize age and age^2 (z-score, then round to 1 decimal place)\ndf[\"age_std\"] = ((df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()).round(1)\ndf[\"age_sq_std\"] = (df[\"age_std\"] ** 2).round(1)\n\n# Design matrix: intercept, standardized age, age^2, region dummies, iscustomer\nX = patsy.dmatrix(\"1 + age_std + age_sq_std + C(region, Treatment(reference='Midwest')) + iscustomer\", df, return_type=\"dataframe\")\nY = df[\"patents\"].values\nX_matrix = X.values\n\n# Define Poisson regression log-likelihood\ndef poisson_regression_loglikelihood(beta, Y, X):\n    Xb = X @ beta\n    lambdas = np.exp(Xb)\n    return np.sum(Y * Xb - lambdas - gammaln(Y + 1))\n\n# Negative log-likelihood for optimization\ndef neg_loglik(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\n# Initial beta guess\ninitial_beta = np.zeros(X_matrix.shape[1])\n\n# Optimize with bounds and BFGS\nresult = minimize(neg_loglik, initial_beta, args=(Y, X_matrix), method=\"BFGS\", options={\"disp\": True})\n\n\nbeta_hat = result.x\nhessian = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian))\n\n\nsummary_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.design_info.column_names)\n\nOptimization terminated successfully.\n         Current function value: 3257.709028\n         Iterations: 18\n         Function evaluations: 243\n         Gradient evaluations: 27\n\n\nThe table below reports the estimated coefficients and standard errors from the Poisson regression model.\n\nThe coefficient on iscustomer is 0.2084 (SE = 0.0310), suggesting that Blueprinty customers are associated with a significantly higher rate of patent approvals, even after controlling for age and region.\nThe negative coefficients on both age_std and age_sq_std suggest a concave (inverted-U) relationship between firm age and patenting activity.\nRegional effects appear small and statistically insignificant relative to the Midwest baseline.\n\n\nsummary_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n1.3422\n0.0397\n\n\nC(region, Treatment(reference='Midwest'))[T.Northeast]\n0.0302\n0.0472\n\n\nC(region, Treatment(reference='Midwest'))[T.Northwest]\n-0.0179\n0.0575\n\n\nC(region, Treatment(reference='Midwest'))[T.South]\n0.0564\n0.0561\n\n\nC(region, Treatment(reference='Midwest'))[T.Southwest]\n0.0514\n0.0501\n\n\nage_std\n-0.0596\n0.0143\n\n\nage_sq_std\n-0.1549\n0.0142\n\n\niscustomer\n0.2084\n0.0310\n\n\n\n\n\n\n\nTo verify our results, we re-estimate the model using Python’s built-in GLM() function from the statsmodels package with a Poisson family.\n\nimport statsmodels.api as sm\nglm_model = sm.GLM(Y, X_matrix, family=sm.families.Poisson())\nglm_results = glm_model.fit()\nglm_summary_df = pd.DataFrame({\n    \"Coefficient (GLM)\": glm_results.params,\n    \"Std. Error (GLM)\": glm_results.bse\n}, index=X.design_info.column_names)\n\n# Summary table\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3257.7\n\n\nDate:\nSat, 07 Jun 2025\nDeviance:\n2142.5\n\n\nTime:\n15:43:48\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1364\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.3422\n0.038\n35.054\n0.000\n1.267\n1.417\n\n\nx1\n0.0302\n0.044\n0.691\n0.489\n-0.055\n0.116\n\n\nx2\n-0.0179\n0.054\n-0.333\n0.739\n-0.123\n0.087\n\n\nx3\n0.0564\n0.053\n1.070\n0.285\n-0.047\n0.160\n\n\nx4\n0.0514\n0.047\n1.089\n0.276\n-0.041\n0.144\n\n\nx5\n-0.0596\n0.015\n-3.967\n0.000\n-0.089\n-0.030\n\n\nx6\n-0.1549\n0.013\n-11.515\n0.000\n-0.181\n-0.129\n\n\nx7\n0.2084\n0.031\n6.743\n0.000\n0.148\n0.269\n\n\n\n\n\nThe coefficient estimates from GLM() match those obtained via our custom maximum likelihood estimation, confirming the correctness of our implementation. Standard errors are also nearly identical, supporting the numerical validity of our Hessian-based uncertainty estimates.\n\n\n\nThe Poisson regression results provide insight into the firm-level factors associated with higher rates of patent awards.\n\nBlueprinty Customers: The coefficient for iscustomer is approximately 0.208, which is statistically significant. Interpreting this in the context of a Poisson model, we exponentiate the coefficient:\n[ e^{0.208} ] This implies that, holding other factors constant, firms using Blueprinty’s software are expected to receive 23% more patents on average than comparable non-customers.\nFirm Age: The negative coefficients on both age_std and age_sq_std suggest a concave relationship between firm age and patent production. Younger firms tend to have increasing returns to experience initially, but beyond a certain point, additional age is associated with diminishing patent activity.\nRegion: The regional dummy variables (relative to the Midwest) show small and statistically insignificant effects. This suggests that after controlling for age and customer status, regional location is not a strong predictor of patenting success in this dataset.\n\nOverall, the model supports Blueprinty’s marketing claim: even after controlling for firm characteristics, their customers tend to achieve higher rates of patent success.\n\n\n\nTo interpret the practical impact of Blueprinty’s software, we simulated two counterfactual scenarios:\n\nX_0: All firms are treated as non-customers (iscustomer = 0)\nX_1: All firms are treated as customers (iscustomer = 1)\n\nUsing the estimated Poisson regression coefficients, we predicted the number of patents for each firm in both scenarios and computed the average difference:\n[ = i ( {i, } - _{i, } ) ]\nThis means that, on average, using Blueprinty’s software is associated with nearly 0.8 more patents per firm over a 5-year period, controlling for firm age and region. This provides strong evidence in support of the software’s effectiveness.\n\nX_0 = X.copy()\nX_1 = X.copy()\n\n\niscustomer_col = [col for col in X.columns if \"iscustomer\" in col][0]\n\n\nX_0[iscustomer_col] = 0\nX_1[iscustomer_col] = 1\n\n\nX0_matrix = X_0.values\nX1_matrix = X_1.values\n\n\ny_pred_0 = np.exp(X0_matrix @ beta_hat)\ny_pred_1 = np.exp(X1_matrix @ beta_hat)\n\n\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\naverage_diff\n\n0.7958740085534443\n\n\n\n\n\nIn this case study, we investigated the relationship between the use of Blueprinty’s software and the number of patents awarded to engineering firms. Using a Poisson regression framework estimated via maximum likelihood, we found strong evidence that Blueprinty customers tend to receive more patents than non-customers—even after controlling for firm age and regional location.\nThe coefficient on the customer indicator was statistically significant and implied a 23% increase in expected patent counts. A counterfactual prediction exercise further suggested that, on average, using Blueprinty’s software is associated with nearly 0.8 additional patents per firm over a 5-year period.\nOverall, the results support the marketing team’s claim that Blueprinty’s product may enhance patent success. However, further research using experimental or longitudinal data would be helpful to more definitively establish causality."
  },
  {
    "objectID": "project/hw2/index.html#airbnb-case-study",
    "href": "project/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nAlthough true booking data is not available, we use the number of reviews as a proxy for the number of bookings, consistent with prior practice.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_airbnb = pd.read_csv(\"./airbnb.csv\")\n\n# Keep only variables of interest\nvars_of_interest = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf_airbnb_clean = df_airbnb[vars_of_interest].dropna()\n\nplt.figure(figsize=(8, 5))\nsns.histplot(df_airbnb_clean[\"number_of_reviews\"], bins=50)\nplt.xlim(0, 100)\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Number of Listings\")\nplt.title(\"Distribution of Airbnb Reviews (Capped at 100)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model\nTo model the number of reviews (as a proxy for bookings), we use a Poisson regression framework. The outcome is the count of reviews, and the predictors include listing characteristics, review scores, and room type.\n\nimport numpy as np\nimport patsy\nimport statsmodels.api as sm\n\n# Transform and encode\ndf_airbnb_clean[\"log_price\"] = np.log(df_airbnb_clean[\"price\"])\ndf_airbnb_clean[\"ibook\"] = (df_airbnb_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n# Design matrix with one-hot encoding for room type\nX_airbnb = patsy.dmatrix(\"1 + log_price + bedrooms + bathrooms + \"\n                         \"review_scores_cleanliness + review_scores_location + \"\n                         \"review_scores_value + ibook + C(room_type)\",\n                         df_airbnb_clean, return_type=\"dataframe\")\nY_airbnb = df_airbnb_clean[\"number_of_reviews\"].values\n\n# Poisson model\npoisson_model = sm.GLM(Y_airbnb, X_airbnb, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\npoisson_results.summary2().tables[1]  # Clean coefficient table\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3.075887\n0.019149\n160.627772\n0.000000e+00\n3.038356\n3.113419\n\n\nC(room_type)[T.Private room]\n0.085839\n0.003377\n25.421373\n1.463838e-142\n0.079221\n0.092457\n\n\nC(room_type)[T.Shared room]\n-0.105034\n0.009105\n-11.535661\n8.721427e-31\n-0.122880\n-0.087188\n\n\nlog_price\n0.134508\n0.002879\n46.712984\n0.000000e+00\n0.128864\n0.140152\n\n\nbedrooms\n0.046661\n0.002038\n22.900255\n4.619038e-116\n0.042667\n0.050655\n\n\nbathrooms\n-0.152019\n0.003742\n-40.620057\n0.000000e+00\n-0.159354\n-0.144684\n\n\nreview_scores_cleanliness\n0.108745\n0.001496\n72.677404\n0.000000e+00\n0.105813\n0.111678\n\n\nreview_scores_location\n-0.097710\n0.001649\n-59.239789\n0.000000e+00\n-0.100943\n-0.094477\n\n\nreview_scores_value\n-0.079629\n0.001820\n-43.757618\n0.000000e+00\n-0.083196\n-0.076063\n\n\nibook\n0.340836\n0.002891\n117.879132\n0.000000e+00\n0.335169\n0.346503\n\n\n\n\n\n\n\n\n\nInterpretation of Results\nThe regression results offer insight into what drives more bookings (as proxied by reviews):\nPrice: The coefficient on log_price is negative, indicating that higher prices are associated with fewer bookings.\nRoom Type: Shared and private rooms tend to receive fewer bookings compared to entire apartments, all else equal.\nCleanliness, Location, Value: Higher review scores on these dimensions are significantly associated with more bookings, especially “value” and “location.”\nInstant Bookable: Listings that are instantly bookable see a meaningful increase in expected bookings, suggesting customers prefer lower friction when booking.\n\n\nMarginal Effect of Instant Bookable\nTo estimate the practical impact of making a listing instantly bookable, we conduct a counterfactual simulation:\nX_0: All listings are set to not instantly bookable (ibook = 0)\nX_1: All listings are set to instantly bookable (ibook = 1)\nWe use the fitted Poisson regression model to predict the number of reviews in both cases and compute the average difference.\n\nX0_airbnb = X_airbnb.copy()\nX1_airbnb = X_airbnb.copy()\n\n# Identify the column corresponding to 'ibook'\nibook_col = [col for col in X_airbnb.columns if \"ibook\" in col][0]\n\n\nX0_airbnb[ibook_col] = 0\nX1_airbnb[ibook_col] = 1\n\n# Predict expected reviews under each scenario\npred_reviews_0 = np.exp(X0_airbnb @ poisson_results.params)\npred_reviews_1 = np.exp(X1_airbnb @ poisson_results.params)\n\naverage_increase = np.mean(pred_reviews_1 - pred_reviews_0)\naverage_increase\n\n7.964686182144014\n\n\nThis simulation suggests that, on average, enabling Instant Bookable leads to approximately 7.96 more reviews per listing, holding all other characteristics constant. This reinforces the importance of reducing booking friction to increase customer engagement on the platform.\n\n\nConclusion\nThis analysis explored what drives variation in the number of Airbnb reviews—used as a proxy for bookings—across listings in New York City. Using Poisson regression, we find that several factors significantly predict higher booking rates:\nPrice is negatively associated with reviews, suggesting that more expensive listings tend to receive fewer bookings.\nReview scores on cleanliness, location, and value are strong predictors of booking volume, especially value and location.\nInstant Bookable status meaningfully increases expected bookings. A counterfactual simulation showed that enabling instant booking is associated with an average increase of roughly 7.96 reviews per listing.\nThese findings suggest that Airbnb hosts can boost their success by improving perceived value, maintaining high review quality, and enabling instant booking where possible."
  },
  {
    "objectID": "hw2_questions.html",
    "href": "hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#blueprinty-case-study",
    "href": "hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#airbnb-case-study",
    "href": "hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. For at least one variable, perform the test as both t-test (use the formula in the class slides) and separately as a linear regression (regress for example mrm2 on treatment); confirm both methods yield the exact same results. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made (you can do this as a bivariate linear regression if you want). It may help to confirm your calculations match Table 2a Panel A. Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or something that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. This average will likely be “noisey” when only averaging a few numbers, but should “settle down” and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms at sample sizes 50, 200, 500, and 1000. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hui’s Website",
    "section": "",
    "text": "Here is a paragraph about me! I am a graduate student in UCSD Rady school\nlast updated 2025-05-07"
  },
  {
    "objectID": "hw3_questions.html",
    "href": "hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "hw3_questions.html#simulate-conjoint-data",
    "href": "hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "hw3_questions.html#preparing-the-data-for-estimation",
    "href": "hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data"
  },
  {
    "objectID": "hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval."
  },
  {
    "objectID": "hw3_questions.html#estimation-via-bayesian-methods",
    "href": "hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach."
  },
  {
    "objectID": "hw3_questions.html#discussion",
    "href": "hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "first-blog\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "HuiL's Website",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n#  treatment + control + mrm2\ndf_subset = df[[\"treatment\", \"control\", \"mrm2\"]].dropna()\n\n# \nmeans = df_subset.groupby(\"treatment\")[\"mrm2\"].mean()\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df_subset[df_subset[\"treatment\"] == 1][\"mrm2\"],\n    df_subset[df_subset[\"treatment\"] == 0][\"mrm2\"],\n    equal_var=False  # Welch's t-test\n)\n\n\nX = sm.add_constant(df_subset[\"treatment\"])\nmodel = sm.OLS(df_subset[\"mrm2\"], X).fit()\n\n\nprint(\"=== mean value ===\")\nprint(means)\nprint(\"\\n=== t-test ===\")\nprint(f\"t = {t_stat:.3f}, p = {p_value:.3f}\")\nprint(\"\\n=== OLS Regression result ===\")\nprint(model.summary())\n\n=== mean value ===\ntreatment\n0    12.998142\n1    13.011828\nName: mrm2, dtype: float64\n\n=== t-test ===\nt = 0.120, p = 0.905\n\n=== OLS Regression result ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\nTime:                        21:28:17   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nimport statsmodels.formula.api as smf\n\n\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\n# t-test \nt_stat, p_value = stats.ttest_ind(\n    df[df[\"treatment\"] == 1][\"gave\"],\n    df[df[\"treatment\"] == 0][\"gave\"],\n    equal_var=False\n)\n\n\nols_model = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Probit \nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\n\n\nfig, ax = plt.subplots(figsize=(6,4))\ngave_by_group.plot(kind='bar', color=[\"pink\", \"blue\"], ax=ax)\nax.set_ylabel(\"Proportion Donated\")\nax.set_title(\"Donation Rate by Treatment Group\")\nax.set_xticklabels([\"Control\", \"Treatment\"], rotation=0)\nax.set_ylim(0, 0.05)\nplt.tight_layout()\n\n\nprint(\"t =\", t_stat, \"p =\", p_value)\n\nprint(ols_model.summary2().tables[1])\n\nprint(probit_model.summary2().tables[1])\n\nt = 3.2094621908279835 p = 0.001330982345091417\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  0.017858  0.001101  16.224643  4.779032e-59  0.015701  0.020016\ntreatment  0.004180  0.001348   3.101361  1.927403e-03  0.001538  0.006822\n              Coef.  Std.Err.         z     P&gt;|z|    [0.025    0.975]\nIntercept -2.100141  0.023316 -90.07277  0.000000 -2.145840 -2.054443\ntreatment  0.086785  0.027879   3.11293  0.001852  0.032143  0.141426\n\n\n\n\n\n\n\n\n\n\n\ndf_matched = df[(df[\"treatment\"] == 1)]\n\n\nrates = df_matched.groupby(\"ratio\")[\"gave\"].mean()\nprint(\"Donation rates by match ratio:\")\nprint(rates)\n\n# t-tests\nfrom scipy.stats import ttest_ind\n\n# 1:1 vs 2:1\np12 = ttest_ind(df_matched[df_matched[\"ratio\"] == 1][\"gave\"],\n                df_matched[df_matched[\"ratio\"] == 2][\"gave\"],\n                equal_var=False)\n\n# 2:1 vs 3:1\np23 = ttest_ind(df_matched[df_matched[\"ratio\"] == 2][\"gave\"],\n                df_matched[df_matched[\"ratio\"] == 3][\"gave\"],\n                equal_var=False)\n\nprint(f\"1:1 vs 2:1 t-test p = {p12.pvalue:.4f}\")\nprint(f\"2:1 vs 3:1 t-test p = {p23.pvalue:.4f}\")\n\nDonation rates by match ratio:\nratio\nControl         NaN\n1          0.020749\n2          0.022633\n3          0.022733\nName: gave, dtype: float64\n1:1 vs 2:1 t-test p = 0.3345\n2:1 vs 3:1 t-test p = 0.9600\n\n\n/tmp/ipykernel_2616514/617877570.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  rates = df_matched.groupby(\"ratio\")[\"gave\"].mean()\n\n\n\n#\ndf[\"ratio1\"] = (df[\"ratio\"] == 1).astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == 2).astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == 3).astype(int)\n\n\ndf_matched = df[df[\"treatment\"] == 1]\n\n# 回归\nimport statsmodels.api as sm\n\nX = df_matched[[\"ratio1\", \"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df_matched[\"gave\"]\n\nmodel = sm.OLS(y, X).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n-1.597075e+09\n3.428594e+11\n-0.004658\n0.996283\n-6.736135e+11\n6.704193e+11\n\n\nratio1\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio2\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\nratio3\n1.597075e+09\n3.428594e+11\n0.004658\n0.996283\n-6.704193e+11\n6.736135e+11\n\n\n\n\n\n\n\n\n# 响应率\nrate_1 = df_matched[df_matched[\"ratio\"] == 1][\"gave\"].mean()\nrate_2 = df_matched[df_matched[\"ratio\"] == 2][\"gave\"].mean()\nrate_3 = df_matched[df_matched[\"ratio\"] == 3][\"gave\"].mean()\n\nprint(f\"1:1 vs 2:1 difference: {rate_2 - rate_1:.4f}\")\nprint(f\"2:1 vs 3:1 difference: {rate_3 - rate_2:.4f}\")\n\n1:1 vs 2:1 difference: 0.0019\n2:1 vs 3:1 difference: 0.0001\n\n\n\nimport statsmodels.api as sm\n\nX = df[\"treatment\"]\nX = sm.add_constant(X)\ny = df[\"amount\"]\n\nmodel = sm.OLS(y, X).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n0.813268\n0.067418\n12.062995\n1.843438e-33\n0.681127\n0.945409\n\n\ntreatment\n0.153605\n0.082561\n1.860503\n6.282029e-02\n-0.008216\n0.315426\n\n\n\n\n\n\n\n\ndf_gave = df[df[\"gave\"] == 1]\n\nX = df_gave[\"treatment\"]\nX = sm.add_constant(X)\ny = df_gave[\"amount\"]\n\nmodel_gave = sm.OLS(y, X).fit()\nmodel_gave.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n45.540268\n2.423378\n18.792063\n5.473578e-68\n40.784958\n50.295579\n\n\ntreatment\n-1.668393\n2.872384\n-0.580839\n5.614756e-01\n-7.304773\n3.967986\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Prepare data\ncontrol_amounts = df[(df[\"treatment\"] == 0) & (df[\"gave\"] == 1)][\"amount\"]\ntreatment_amounts = df[(df[\"treatment\"] == 1) & (df[\"gave\"] == 1)][\"amount\"]\n\n# Plot for control group\nfig, ax = plt.subplots()\nax.hist(control_amounts, bins=30, alpha=0.7, label=\"Control\", color=\"skyblue\")\nax.axvline(control_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Control Mean\")\nax.set_title(\"Donation Amounts: Control Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig\n\n# Plot for treatment group\nfig, ax = plt.subplots()\nax.hist(treatment_amounts, bins=30, alpha=0.7, label=\"Treatment\", color=\"lightgreen\")\nax.axvline(treatment_amounts.mean(), color=\"red\", linestyle=\"--\", label=\"Treatment Mean\")\nax.set_title(\"Donation Amounts: Treatment Group\")\nax.set_xlabel(\"Amount\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set probabilities\np_control = 0.018\np_treatment = 0.022\nn = 10_000\n\n# Simulate donation outcomes\nnp.random.seed(42)\ncontrol = np.random.binomial(n=1, p=p_control, size=n)\ntreatment = np.random.binomial(n=1, p=p_treatment, size=n)\n\n# Calculate difference vector and cumulative average\ndiff = treatment - control\ncumulative_avg = np.cumsum(diff) / np.arange(1, n + 1)\n\n# Plot cumulative average\nfig, ax = plt.subplots()\nax.plot(cumulative_avg, label=\"Cumulative Average\")\nax.axhline(y=0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nax.set_title(\"Law of Large Numbers: Cumulative Average of Differences\")\nax.set_xlabel(\"Sample Size\")\nax.set_ylabel(\"Cumulative Average\")\nax.legend()\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed\nnp.random.seed(123)\n\n# Simulation parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\n\n# Create 4 histograms\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(1000):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treatment, n)\n        diff = treatment.mean() - control.mean()\n        differences.append(diff)\n\n    ax = axs[i // 2, i % 2]\n    ax.hist(differences, bins=30, color=\"grey\", edgecolor=\"black\")\n    ax.set_title(f\"Sample Size: {n}\")\n    ax.set_xlabel(\"Average Difference\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nfig"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/first-blog/index.html",
    "href": "blog/first-blog/index.html",
    "title": "first-blog",
    "section": "",
    "text": "This is my first blog post. I’m excited to share my work with the world!"
  },
  {
    "objectID": "hw4_questions.html",
    "href": "hw4_questions.html",
    "title": "Add Title",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "hw4_questions.html#a.-k-means",
    "href": "hw4_questions.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "hw4_questions.html#b.-latent-class-mnl",
    "href": "hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "hw4_questions.html#a.-k-nearest-neighbors",
    "href": "hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "hw4_questions.html#b.-key-drivers-analysis",
    "href": "hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "hw2.html",
    "href": "hw2.html",
    "title": "HuiL's Website",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_cvs(\"MGTA495-872416/project/hw2/blueprinty.csv\")\n\ndf.info(), df.head()\n\nAttributeError: module 'pandas' has no attribute 'read_cvs'\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up the style\nsns.set(style=\"whitegrid\")\n\n# Calculate group means\nmean_patents = df.groupby(\"iscustomer\")[\"patents\"].mean().rename({0: \"Non-Customers\", 1: \"Customers\"})\n\n# Create histogram\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"stack\", palette=\"Set2\", bins=20)\nplt.xlabel(\"Number of Patents (Last 5 Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Distribution of Patents by Blueprinty Customer Status\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\nmean_patents"
  },
  {
    "objectID": "project/hw3/index.html",
    "href": "project/hw3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "project/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "project/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "project/hw3/index.html#simulate-conjoint-data",
    "href": "project/hw3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed."
  },
  {
    "objectID": "project/hw3/index.html#preparing-the-data-for-estimation",
    "href": "project/hw3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\n\n\n\n\nRead Data\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./conjoint_data.csv\")\n\ndf.info(), df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   resp    3000 non-null   int64 \n 1   task    3000 non-null   int64 \n 2   choice  3000 non-null   int64 \n 3   brand   3000 non-null   object\n 4   ad      3000 non-null   object\n 5   price   3000 non-null   int64 \ndtypes: int64(4), object(2)\nmemory usage: 140.8+ KB\n\n\n(None,\n    resp  task  choice brand   ad  price\n 0     1     1       1     N  Yes     28\n 1     1     1       0     H  Yes     16\n 2     1     1       0     P  Yes     16\n 3     1     2       0     N  Yes     32\n 4     1     2       1     P  Yes     16)\n\n\n\n\n\n\nData Summary\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nresp\nRespondent ID\n\n\ntask\nTask number answered by each respondent (3 options per task)\n\n\nchoice\nWhether the option was chosen (1 = chosen, 0 = not chosen)\n\n\nbrand\nBrand (categorical: N, H, P)\n\n\nad\nWhether the product has advertising (Yes/No)\n\n\nprice\nProduct price\n\n\n\n\n\nOne-hot Encode Categorical Variables\n\n# One-hot encode brand and ad\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Check if each (resp, task) pair has 3 alternatives\ngroup_counts = df_encoded.groupby(['resp', 'task']).size().value_counts()\n\n# Preview reshaped data\ndf_encoded.head(), group_counts\n\n(   resp  task  choice  price  brand_N  brand_P  ad_Yes\n 0     1     1       1     28     True    False    True\n 1     1     1       0     16    False    False    True\n 2     1     1       0     16    False     True    True\n 3     1     2       0     32     True    False    True\n 4     1     2       1     16    False     True    True,\n 3    1000\n Name: count, dtype: int64)\n\n\nWe one-hot encode the categorical variables brand and ad to prepare them for use in the multinomial logit model, which requires numeric input features. We also confirm that each respondent-task pair includes exactly 3 alternatives, ensuring proper setup for MNL estimation. ### The dataset is now encoded and validated, ready for modeling."
  },
  {
    "objectID": "project/hw3/index.html#estimation-via-maximum-likelihood",
    "href": "project/hw3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe estimate the parameters of the multinomial logit (MNL) model using maximum likelihood, via scipy.optimize.minimize with the BFGS method.\nThe log-likelihood function is constructed over respondent-task-choice groups, treating the utility of each product as a linear function of the features:\n\\[\nU_{ij} = X_{ij} \\beta\n\\]\nWe compute the negative log-likelihood, then solve for parameter estimates by minimizing this objective. We also compute the inverse Hessian to obtain standard errors and 95% confidence intervals.\n\n\n\n\n\n\nMaximum Likelihood Estimation Results\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\n\n# Load and encode data\ndf = pd.read_csv(\"./conjoint_data.csv\")\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Define X and y\nX = df_encoded[[\"price\", \"brand_N\", \"brand_P\", \"ad_Yes\"]].values\ny = df_encoded[\"choice\"].values\ngroups = df_encoded[[\"resp\", \"task\"]].values\n\n# Ensure 3 alternatives per task\ngroup_sizes = pd.DataFrame(groups, columns=[\"resp\", \"task\"]).value_counts().values\nassert all(group_sizes == 3)\n\n# Reshape\nX_grouped = X.reshape((-1, 3, X.shape[1]))\ny_grouped = y.reshape((-1, 3))\n\n# Ensure correct types\nX_grouped = np.asarray(X_grouped, dtype=np.float64)\ny_grouped = np.asarray(y_grouped, dtype=np.float64)\n\n# Define log-likelihood\ndef neg_log_likelihood(beta):\n    beta = np.asarray(beta, dtype=np.float64)\n    utilities = np.dot(X_grouped, beta)  # Shape: (N_tasks, 3)\n    exp_util = np.exp(utilities)\n    probs = exp_util / exp_util.sum(axis=1, keepdims=True)\n    chosen_probs = (probs * y_grouped).sum(axis=1)\n    log_likelihood = np.log(chosen_probs + 1e-12).sum()\n    return -log_likelihood\n\n# Estimate\nbeta0 = np.zeros(X.shape[1])\nresult = minimize(neg_log_likelihood, beta0, method=\"BFGS\")\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n# Standard errors and confidence intervals\nse = np.sqrt(np.diag(hessian_inv))\nci_lower = beta_hat - 1.96 * se\nci_upper = beta_hat + 1.96 * se\n\n# Output results\nresults_df = pd.DataFrame({\n    \"Parameter\": [\"price\", \"brand_N\", \"brand_P\", \"ad_Yes\"],\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se,\n    \"95% CI Lower\": ci_lower,\n    \"95% CI Upper\": ci_upper\n})\nresults_df\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nprice\n-0.099480\n0.006342\n-0.111911\n-0.087050\n\n\n1\nbrand_N\n0.941195\n0.118714\n0.708515\n1.173875\n\n\n2\nbrand_P\n0.501616\n0.121462\n0.263551\n0.739681\n\n\n3\nad_Yes\n-0.731994\n0.089000\n-0.906434\n-0.557554\n\n\n\n\n\n\n\n\n\n\nThe table below shows the Maximum Likelihood Estimates, standard errors, and 95% confidence intervals for each parameter:\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nprice\n-0.0995\n0.0063\n-0.1119\n-0.0871\n\n\nbrand_N\n0.9412\n0.1187\n0.7085\n1.1739\n\n\nbrand_P\n0.5016\n0.1215\n0.2636\n0.7397\n\n\nad_Yes\n-0.7320\n0.0890\n-0.9064\n-0.5576\n\n\n\n\nInterpretation:\nPrice has a statistically significant negative coefficient, confirming that higher prices reduce the probability of selection.\nBrand_N and Brand_P have positive coefficients, suggesting they are preferred over the reference brand (likely Brand H).\nAdvertising (ad_Yes) has a negative coefficient, indicating that advertising may reduce product utility in this context."
  },
  {
    "objectID": "project/hw3/index.html#estimation-via-bayesian-methods",
    "href": "project/hw3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\n\n\n\n\nBayesian Posterior Summary for the 4 Parameters\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reload and reshape data\ndf = pd.read_csv(\"./conjoint_data.csv\")\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\nX = df_encoded[[\"price\", \"brand_N\", \"brand_P\", \"ad_Yes\"]].values\ny = df_encoded[\"choice\"].values\n\nX_grouped = X.reshape((-1, 3, X.shape[1]))\ny_grouped = y.reshape((-1, 3))\nX_grouped = np.asarray(X_grouped, dtype=np.float64)\ny_grouped = np.asarray(y_grouped, dtype=np.float64)\n\n# Log-likelihood function\ndef log_likelihood(beta):\n    utilities = np.dot(X_grouped, beta)\n    exp_util = np.exp(utilities)\n    probs = exp_util / exp_util.sum(axis=1, keepdims=True)\n    chosen_probs = (probs * y_grouped).sum(axis=1)\n    return np.log(chosen_probs + 1e-12).sum()\n\n# Log-prior: N(0,1) for price; N(0,5) for others\ndef log_prior(beta):\n    return -0.5 * (beta[0]**2 / 1**2 + np.sum(beta[1:]**2 / 5**2))\n\n# Log posterior\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# MCMC settings\nn_iter = 11000\nburn_in = 1000\nbeta_dim = 4\nsamples = np.zeros((n_iter, beta_dim))\ncurrent_beta = np.zeros(beta_dim)\ncurrent_log_post = log_posterior(current_beta)\n\n# Proposal distribution SDs: [0.005, 0.05, 0.05, 0.05]\nproposal_sd = np.array([0.005, 0.05, 0.05, 0.05])\n\n# MCMC sampling\nnp.random.seed(0)\nfor i in range(n_iter):\n    proposal = current_beta + np.random.normal(0, proposal_sd)\n    proposal_log_post = log_posterior(proposal)\n    accept_prob = np.exp(proposal_log_post - current_log_post)\n    if np.random.rand() &lt; accept_prob:\n        current_beta = proposal\n        current_log_post = proposal_log_post\n    samples[i, :] = current_beta\n\n# Remove burn-in\nsamples_post = samples[burn_in:]\n\n# Summary statistics\nposterior_df = pd.DataFrame(samples_post, columns=[\"price\", \"brand_N\", \"brand_P\", \"ad_Yes\"])\nsummary_df = posterior_df.describe(percentiles=[0.025, 0.975]).T[[\"mean\", \"std\"]]\nsummary_df[\"CI Lower\"] = posterior_df.quantile(0.025)\nsummary_df[\"CI Upper\"] = posterior_df.quantile(0.975)\n\nsummary_df\n\n\n\n\n\n\n\n\nmean\nstd\nCI Lower\nCI Upper\n\n\n\n\nprice\n-0.099777\n0.006504\n-0.113024\n-0.087153\n\n\nbrand_N\n0.938162\n0.110142\n0.724612\n1.160254\n\n\nbrand_P\n0.499689\n0.107640\n0.297368\n0.727317\n\n\nad_Yes\n-0.727447\n0.090435\n-0.902497\n-0.541023\n\n\n\n\n\n\n\n\n\n\n\nBayesian Posterior Summary for the 4 Parameters\n\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nStd. Dev\n95% Credible Interval Lower\n95% Credible Interval Upper\n\n\n\n\nprice\n-0.0998\n0.0065\n-0.1130\n-0.0872\n\n\nbrand_N\n0.9382\n0.1101\n0.7246\n1.1603\n\n\nbrand_P\n0.4997\n0.1076\n0.2974\n0.7273\n\n\nad_Yes\n-0.7274\n0.0904\n-0.9025\n-0.5410\n\n\n\n\n\nPosterior Distributions and Trace Plots\nThe following figure shows the trace plots and posterior histograms for each of the four parameters estimated via Metropolis-Hastings:\n\nfig, axes = plt.subplots(4, 2, figsize=(8, 8))\nparam_names = [\"price\", \"brand_N\", \"brand_P\", \"ad_Yes\"]\n\nfor i, param in enumerate(param_names):\n    axes[i, 0].plot(posterior_df[param])\n    axes[i, 0].set_title(f\"Trace Plot: {param}\")\n    axes[i, 0].set_xlabel(\"Iteration\")\n    axes[i, 0].set_ylabel(\"Value\")\n\n    axes[i, 1].hist(posterior_df[param], bins=30, edgecolor='black')\n    axes[i, 1].set_title(f\"Posterior Histogram: {param}\")\n    axes[i, 1].set_xlabel(\"Value\")\n    axes[i, 1].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nMLE Vs Bayesian Estimates\n\ncompare_df = pd.DataFrame({\n    \"Parameter\": [\"price\", \"brand_N\", \"brand_P\", \"ad_Yes\"],\n    \"MLE Estimate\": [-0.0995, 0.9412, 0.5016, -0.7320],\n    \"Bayes Mean\": [-0.0998, 0.9382, 0.4997, -0.7274]\n})\n\n\ncompare_df[\"Difference\"] = compare_df[\"Bayes Mean\"] - compare_df[\"MLE Estimate\"]\ncompare_df[\"% Difference\"] = 100 * compare_df[\"Difference\"] / compare_df[\"MLE Estimate\"]\n\ncompare_df\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\nBayes Mean\nDifference\n% Difference\n\n\n\n\n0\nprice\n-0.0995\n-0.0998\n-0.0003\n0.301508\n\n\n1\nbrand_N\n0.9412\n0.9382\n-0.0030\n-0.318742\n\n\n2\nbrand_P\n0.5016\n0.4997\n-0.0019\n-0.378788\n\n\n3\nad_Yes\n-0.7320\n-0.7274\n0.0046\n-0.628415\n\n\n\n\n\n\n\nThe table above provides a side-by-side comparison of parameter estimates from MLE and Bayesian methods. As shown, the differences across all four parameters are very small, generally under 0.005 in magnitude, and the percent differences stay within ±0.6%, suggesting that the posterior means are closely aligned with MLE results. This outcome indicates that the priors used in the Bayesian estimation were weakly informative, having minimal regularization effect, and reinforces the consistency of both estimation approaches on this dataset.\n\n\nComparison of MLE and Bayesian Estimation Results\nBoth the Maximum Likelihood Estimation (MLE) and Bayesian methods provide consistent insights about the effects of the product features on choice behavior. Below is a comparison of their outputs:\n\nParameter Estimates: The posterior means from the Bayesian estimation are very close to the MLE point estimates across all four parameters. For example, the coefficient on price is approximately -0.0998 in both methods, indicating a consistent negative effect of price on choice probability.\nUncertainty Quantification:\n\nMLE provides standard errors and 95% confidence intervals based on the inverse Hessian.\nBayesian estimation yields posterior standard deviations and 95% credible intervals from the posterior sample distribution.\nNotably, the width and range of the Bayesian credible intervals are slightly more flexible, especially under weakly informative priors.\n\nRegularization Effect of Priors:\n\nThe Bayesian method includes prior distributions (N(0,1) for price and N(0,5) for binary variables), which can slightly “shrink” estimates compared to MLE, especially in small samples. However, in this case, the priors are relatively weak, so their influence is modest.\n\nInterpretability:\n\nMLE intervals are interpreted in terms of repeated sampling (frequentist), whereas Bayesian intervals can be directly interpreted as probabilities (e.g., “There is a 95% chance the parameter lies in this interval”).\n\n\nIn conclusion, both estimation techniques yield very similar results in this well-specified, clean dataset. The Bayesian framework provides a richer view by revealing the full posterior distribution, while MLE is computationally faster and commonly used for large datasets."
  },
  {
    "objectID": "project/hw3/index.html#discussion",
    "href": "project/hw3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpreting the Parameter Estimates\nEven though we did not simulate the data ourselves, we can still interpret the meaning of the estimated parameters:\n\nThe negative coefficient on price (β_price &lt; 0) makes intuitive sense: as price increases, the utility of a product decreases, making consumers less likely to choose it. This aligns with standard economic theory.\nThe fact that β_Netflix &gt; β_Prime (i.e., the coefficient on the brand_N dummy is larger than that on brand_P) suggests that, on average, respondents prefer the Netflix-branded product over the Prime-branded one, all else equal. This may be due to brand loyalty, perceived quality, or other unobserved factors favoring Netflix.\nSimilarly, if brand_Hulu is the omitted category, then both brand_N and brand_P are being compared relative to Hulu. So we can infer a ranking of brand preferences from the sign and magnitude of the coefficients.\n\n\n\nToward a Hierarchical (Random Coefficient) Model\nThe current multinomial logit (MNL) model assumes that all consumers share the same coefficients (i.e., preferences). However, in the real world, consumers are heterogeneous — for example, some are more price-sensitive than others, or may strongly prefer one brand over another.\nTo capture this individual-level variation, we can move to a hierarchical Bayesian model (also called a mixed logit or random coefficient model):\n\nInstead of estimating a single β vector, we assume each consumer i has their own vector β_i, drawn from a common population distribution (e.g., multivariate normal).\nThe model becomes two-layered:\n\nLevel 1 (individual): choice probabilities are modeled as in the MNL, but using β_i.\nLevel 2 (population): we place priors on the distribution of β_i (e.g., mean and covariance matrix), and estimate those hyperparameters from the data.\n\nTechnically, this would require:\n\nModifying the likelihood function to sum/integrate over the distribution of random coefficients,\nUsing more advanced sampling methods like Gibbs sampling or Hamiltonian Monte Carlo to perform posterior inference.\n\n\nSuch models are widely used in practice, especially in marketing research, to generate individual-level predictions, segment consumers, and simulate responses to new product designs."
  },
  {
    "objectID": "project/projects.html",
    "href": "project/projects.html",
    "title": "My Homework",
    "section": "",
    "text": "HW4 Part 1: Unsupervised Learning for Customer Segmentation\n\n\n\n\n\n\nHui Liu\n\n\nJun 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHW4 Part 2: Supervised Learning for Driver Analysis\n\n\n\n\n\n\nHui Liu\n\n\nJun 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nHui Liu\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nHui Liu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nHui Liu\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/hw4/unsupervised.html",
    "href": "project/hw4/unsupervised.html",
    "title": "HW4 Part 1: Unsupervised Learning for Customer Segmentation",
    "section": "",
    "text": "This analysis demonstrates how unsupervised learning can be used to segment customers (or in this case, penguins) based on physical characteristics, which is analogous to segmenting markets in marketing analytics.\n\n\nWe used the Palmer Penguins dataset and selected two numerical features:\n\nbill_length_mm\nflipper_length_mm\n\nAfter removing missing values, we applied our custom implementation of the K-Means clustering algorithm.\n\n\n\n\n\n\nRead Data\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"./palmer_penguins.csv\")\n\n\ndf_clean = df.dropna(subset=[\"bill_length_mm\", \"flipper_length_mm\"])\n\n\nX = df_clean[[\"bill_length_mm\", \"flipper_length_mm\"]].values\n\ndf_clean.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe implemented the K-Means algorithm from scratch using Euclidean distance for assignment and centroid updates based on the mean. The algorithm converged after 9 iterations.\nBelow is a GIF animation showing how the clustering evolves across iterations:\n\n\n\nK-Means Clustering Animation\n\n\n\n\n\n\n\n\nCustom K-Means Algorithm Implementation\n\n\n\n\n\n\nimport numpy as np\nimport os\n\n# Set random seed\nnp.random.seed(42)\n\n# Create a folder to save the step images\nframe_dir = \"./kmeans_frames\"\nos.makedirs(frame_dir, exist_ok=True)\n\n# Helper: compute Euclidean distance\ndef euclidean_distance(a, b):\n    return np.linalg.norm(a - b, axis=1)\n\n# Save each step's plot\ndef save_kmeans_step_plot(X, labels, centroids, step, frame_dir):\n    plt.figure(figsize=(6, 5))\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set2', s=40, edgecolor='k')\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=120, marker='x', linewidths=2)\n    plt.title(f\"K-Means Iteration {step}\")\n    plt.xlabel(\"Bill Length (mm)\")\n    plt.ylabel(\"Flipper Length (mm)\")\n    plt.tight_layout()\n    plt.savefig(f\"{frame_dir}/frame_{step:02d}.png\")\n    plt.close()\n\n# Custom K-Means implementation with visualization saving\ndef custom_kmeans(X, k=3, max_iter=10, save_dir=None):\n    n_samples, n_features = X.shape\n    initial_indices = np.random.choice(n_samples, k, replace=False)\n    centroids = X[initial_indices]\n    labels = np.zeros(n_samples)\n    \n    for step in range(max_iter):\n        # Assign clusters\n        for i in range(n_samples):\n            distances = euclidean_distance(X[i].reshape(1, -1), centroids)\n            labels[i] = np.argmin(distances)\n        \n        # Save current step\n        if save_dir:\n            save_kmeans_step_plot(X, labels, centroids, step, save_dir)\n        \n        # Compute new centroids\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        \n        # Check for convergence\n        if np.allclose(centroids, new_centroids):\n            print(f\"Converged at step {step}\")\n            break\n        \n        centroids = new_centroids\n    \n    # Final plot\n    if save_dir:\n        save_kmeans_step_plot(X, labels, centroids, step + 1, save_dir)\n\n    return labels, centroids\n\n# Run custom K-Means and save frames\nlabels, centroids = custom_kmeans(X, k=3, max_iter=10, save_dir=frame_dir)\n\nConverged at step 9\n\n\n\n\n\n\n\n\n\n\n\nCreate GIF Animation with imageio\n\n\n\n\n\n\nimport imageio\n\n# Generate GIF from saved frames\nframe_files = sorted([os.path.join(frame_dir, f) for f in os.listdir(frame_dir) if f.endswith(\".png\")])\ngif_path = \"./kmeans_animation.gif\"\n\n# Create gif\nwith imageio.get_writer(gif_path, mode='I', duration=5, loop=0) as writer:\n    for filename in frame_files:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n\ngif_path\n\n/tmp/ipykernel_224466/3049498779.py:10: DeprecationWarning:\n\nStarting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n\n\n\n'./kmeans_animation.gif'\n\n\n\n\n\n\n\n\nTo determine the optimal number of clusters (K), we computed:\n\nWithin-Cluster Sum of Squares (WCSS)\nSilhouette Score\n\nWhile K=2 has the highest Silhouette score, K=3 provides a clearer elbow point in the WCSS curve and still maintains a strong Silhouette score, making it a better overall choice for balancing intra-cluster cohesion and inter-cluster separation. So, both metrics suggested K = 3 as the best choice:\n\n\n\nK-Means evaluation\n\n\n\n\n\n\n\n\nEvaluate Cluster Quality (WCSS & Silhouette)\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\nwcss = []\nsilhouette_scores = []\nK_range = range(2, 8)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X)\n    wcss.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, labels))\n\n# Plot WCSS and Silhouette Scores\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].plot(K_range, wcss, marker='o')\nax[0].set_title('Within-Cluster Sum of Squares (WCSS)')\nax[0].set_xlabel('Number of Clusters (K)')\nax[0].set_ylabel('WCSS')\n\nax[1].plot(K_range, silhouette_scores, marker='o', color='green')\nax[1].set_title('Silhouette Score')\nax[1].set_xlabel('Number of Clusters (K)')\nax[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.savefig(\"kmeans_evaluation_vertical.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo validate our custom implementation, we compared the clustering result with scikit-learn’s built-in KMeans function. The cluster assignments and centroid locations are nearly identical, confirming the correctness of our algorithm.\n\n\n\nK-Means Sklearn\n\n\n\n\n\n\n\n\nScikit-learn KMeans Comparison\n\n\n\n\n\n\n# Fit KMeans from scikit-learn for K=3\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42, n_init=10)\nlabels_sklearn = kmeans_sklearn.fit_predict(X)\ncentroids_sklearn = kmeans_sklearn.cluster_centers_\n\n# Plot sklearn KMeans result\nplt.figure(figsize=(6, 5))\nplt.scatter(X[:, 0], X[:, 1], c=labels_sklearn, cmap='Set2', s=40, edgecolor='k')\nplt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], c='black', s=120, marker='x', linewidths=2)\nplt.title(\"K-Means Clustering (scikit-learn)\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.tight_layout()\nplt.savefig(\"kmeans_sklearn_clean.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur custom K-Means algorithm successfully clustered penguins into three meaningful groups using only bill length and flipper length. Evaluation metrics and visual inspection both suggest that K=3 provides a good balance of cohesion and separation. The results also validate well against the standard scikit-learn implementation."
  },
  {
    "objectID": "project/hw4/unsupervised.html#a.-k-means-clustering-on-palmer-penguins",
    "href": "project/hw4/unsupervised.html#a.-k-means-clustering-on-palmer-penguins",
    "title": "HW4 Part 1: Unsupervised Learning for Customer Segmentation",
    "section": "",
    "text": "This analysis demonstrates how unsupervised learning can be used to segment customers (or in this case, penguins) based on physical characteristics, which is analogous to segmenting markets in marketing analytics.\n\n\nWe used the Palmer Penguins dataset and selected two numerical features:\n\nbill_length_mm\nflipper_length_mm\n\nAfter removing missing values, we applied our custom implementation of the K-Means clustering algorithm.\n\n\n\n\n\n\nRead Data\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"./palmer_penguins.csv\")\n\n\ndf_clean = df.dropna(subset=[\"bill_length_mm\", \"flipper_length_mm\"])\n\n\nX = df_clean[[\"bill_length_mm\", \"flipper_length_mm\"]].values\n\ndf_clean.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe implemented the K-Means algorithm from scratch using Euclidean distance for assignment and centroid updates based on the mean. The algorithm converged after 9 iterations.\nBelow is a GIF animation showing how the clustering evolves across iterations:\n\n\n\nK-Means Clustering Animation\n\n\n\n\n\n\n\n\nCustom K-Means Algorithm Implementation\n\n\n\n\n\n\nimport numpy as np\nimport os\n\n# Set random seed\nnp.random.seed(42)\n\n# Create a folder to save the step images\nframe_dir = \"./kmeans_frames\"\nos.makedirs(frame_dir, exist_ok=True)\n\n# Helper: compute Euclidean distance\ndef euclidean_distance(a, b):\n    return np.linalg.norm(a - b, axis=1)\n\n# Save each step's plot\ndef save_kmeans_step_plot(X, labels, centroids, step, frame_dir):\n    plt.figure(figsize=(6, 5))\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set2', s=40, edgecolor='k')\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=120, marker='x', linewidths=2)\n    plt.title(f\"K-Means Iteration {step}\")\n    plt.xlabel(\"Bill Length (mm)\")\n    plt.ylabel(\"Flipper Length (mm)\")\n    plt.tight_layout()\n    plt.savefig(f\"{frame_dir}/frame_{step:02d}.png\")\n    plt.close()\n\n# Custom K-Means implementation with visualization saving\ndef custom_kmeans(X, k=3, max_iter=10, save_dir=None):\n    n_samples, n_features = X.shape\n    initial_indices = np.random.choice(n_samples, k, replace=False)\n    centroids = X[initial_indices]\n    labels = np.zeros(n_samples)\n    \n    for step in range(max_iter):\n        # Assign clusters\n        for i in range(n_samples):\n            distances = euclidean_distance(X[i].reshape(1, -1), centroids)\n            labels[i] = np.argmin(distances)\n        \n        # Save current step\n        if save_dir:\n            save_kmeans_step_plot(X, labels, centroids, step, save_dir)\n        \n        # Compute new centroids\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        \n        # Check for convergence\n        if np.allclose(centroids, new_centroids):\n            print(f\"Converged at step {step}\")\n            break\n        \n        centroids = new_centroids\n    \n    # Final plot\n    if save_dir:\n        save_kmeans_step_plot(X, labels, centroids, step + 1, save_dir)\n\n    return labels, centroids\n\n# Run custom K-Means and save frames\nlabels, centroids = custom_kmeans(X, k=3, max_iter=10, save_dir=frame_dir)\n\nConverged at step 9\n\n\n\n\n\n\n\n\n\n\n\nCreate GIF Animation with imageio\n\n\n\n\n\n\nimport imageio\n\n# Generate GIF from saved frames\nframe_files = sorted([os.path.join(frame_dir, f) for f in os.listdir(frame_dir) if f.endswith(\".png\")])\ngif_path = \"./kmeans_animation.gif\"\n\n# Create gif\nwith imageio.get_writer(gif_path, mode='I', duration=5, loop=0) as writer:\n    for filename in frame_files:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n\ngif_path\n\n/tmp/ipykernel_224466/3049498779.py:10: DeprecationWarning:\n\nStarting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n\n\n\n'./kmeans_animation.gif'\n\n\n\n\n\n\n\n\nTo determine the optimal number of clusters (K), we computed:\n\nWithin-Cluster Sum of Squares (WCSS)\nSilhouette Score\n\nWhile K=2 has the highest Silhouette score, K=3 provides a clearer elbow point in the WCSS curve and still maintains a strong Silhouette score, making it a better overall choice for balancing intra-cluster cohesion and inter-cluster separation. So, both metrics suggested K = 3 as the best choice:\n\n\n\nK-Means evaluation\n\n\n\n\n\n\n\n\nEvaluate Cluster Quality (WCSS & Silhouette)\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\nwcss = []\nsilhouette_scores = []\nK_range = range(2, 8)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X)\n    wcss.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, labels))\n\n# Plot WCSS and Silhouette Scores\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nax[0].plot(K_range, wcss, marker='o')\nax[0].set_title('Within-Cluster Sum of Squares (WCSS)')\nax[0].set_xlabel('Number of Clusters (K)')\nax[0].set_ylabel('WCSS')\n\nax[1].plot(K_range, silhouette_scores, marker='o', color='green')\nax[1].set_title('Silhouette Score')\nax[1].set_xlabel('Number of Clusters (K)')\nax[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.savefig(\"kmeans_evaluation_vertical.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo validate our custom implementation, we compared the clustering result with scikit-learn’s built-in KMeans function. The cluster assignments and centroid locations are nearly identical, confirming the correctness of our algorithm.\n\n\n\nK-Means Sklearn\n\n\n\n\n\n\n\n\nScikit-learn KMeans Comparison\n\n\n\n\n\n\n# Fit KMeans from scikit-learn for K=3\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42, n_init=10)\nlabels_sklearn = kmeans_sklearn.fit_predict(X)\ncentroids_sklearn = kmeans_sklearn.cluster_centers_\n\n# Plot sklearn KMeans result\nplt.figure(figsize=(6, 5))\nplt.scatter(X[:, 0], X[:, 1], c=labels_sklearn, cmap='Set2', s=40, edgecolor='k')\nplt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], c='black', s=120, marker='x', linewidths=2)\nplt.title(\"K-Means Clustering (scikit-learn)\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.tight_layout()\nplt.savefig(\"kmeans_sklearn_clean.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur custom K-Means algorithm successfully clustered penguins into three meaningful groups using only bill length and flipper length. Evaluation metrics and visual inspection both suggest that K=3 provides a good balance of cohesion and separation. The results also validate well against the standard scikit-learn implementation."
  },
  {
    "objectID": "project/hw4/supervised.html",
    "href": "project/hw4/supervised.html",
    "title": "HW4 Part 2: Supervised Learning for Predictive Modeling",
    "section": "",
    "text": "Generate Training Data and Test Data\n\n\n\n\n\n\n# Re-execute after kernel reset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Generate training data\nnp.random.seed(42)\nn_train = 100\nx1_train = np.random.uniform(-3, 3, n_train)\nx2_train = np.random.uniform(-3, 3, n_train)\nboundary_train = np.sin(4 * x1_train) + x1_train\ny_train = (x2_train &gt; boundary_train).astype(int)\n\ntrain_data = pd.DataFrame({\n    \"x1\": x1_train,\n    \"x2\": x2_train,\n    \"y\": y_train\n})\n\n# Generate test data\nnp.random.seed(99)\nn_test = 100\nx1_test = np.random.uniform(-3, 3, n_test)\nx2_test = np.random.uniform(-3, 3, n_test)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test &gt; boundary_test).astype(int)\n\ntest_data = pd.DataFrame({\n    \"x1\": x1_test,\n    \"x2\": x2_test,\n    \"y\": y_test\n})\n\n\n\n\n\n\n\n\n\n\nPlot training data and wiggly decision boundary\n\n\n\n\n\n\nplt.figure(figsize=(6, 5))\nplt.scatter(x1_train, x2_train, c=y_train, cmap='coolwarm', s=40, edgecolor='k')\nx_curve = np.linspace(-3, 3, 300)\ny_curve = np.sin(4 * x_curve) + x_curve\nplt.plot(x_curve, y_curve, color='black', linestyle='--', label='Decision Boundary')\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Synthetic Training Data with Wiggly Boundary\")\nplt.legend()\nplt.tight_layout()\ntrain_plot_path = \"./knn_train_plot.png\"\nplt.savefig(train_plot_path)\nplt.close()\n\ntrain_plot_path\n\n'./knn_train_plot.png'\n\n\n\n\n\n\nEvaluating K for KNN\nWe ran our custom KNN implementation for values of ( k ) ranging from 1 to 30.\nFor each ( k ), we predicted the test set and recorded the accuracy.\nThe plot below shows how accuracy varies with ( k ):\n\n\n\nKNN Accuracy Curve\n\n\nAs shown, the optimal value of ( k ) appears to be K = X, where accuracy peaks.\n\n\n\n\n\n\nImplement custom KNN classifier\n\n\n\n\n\n\nfrom scipy.spatial import distance\n\ndef knn_predict(X_train, y_train, X_test, k=3):\n    y_pred = []\n    for test_point in X_test:\n        dists = distance.cdist([test_point], X_train)[0]\n        nearest_indices = np.argsort(dists)[:k]\n        nearest_labels = y_train[nearest_indices]\n        majority_vote = np.argmax(np.bincount(nearest_labels))\n        y_pred.append(majority_vote)\n    return np.array(y_pred)\n\n# Prepare train and test arrays\nX_train = train_data[[\"x1\", \"x2\"]].values\ny_train = train_data[\"y\"].values\nX_test = test_data[[\"x1\", \"x2\"]].values\ny_test = test_data[\"y\"].values\n\n# Evaluate accuracy for k = 1 to 30\naccuracies = []\nk_values = list(range(1, 31))\n\nfor k in k_values:\n    preds = knn_predict(X_train, y_train, X_test, k=k)\n    acc = (preds == y_test).mean()\n    accuracies.append(acc)\n\n# Plot accuracy vs K\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, accuracies, marker='o')\nplt.title(\"KNN Accuracy on Test Set\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"./knn_accuracy_curve.png\")\nplt.close()\n\n\"/mnt/data/knn_accuracy_curve.png\"\n\n'/mnt/data/knn_accuracy_curve.png'"
  }
]